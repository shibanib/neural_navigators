{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Navigators: Decoding Neural Circuits for Decision Making\n",
    "\n",
    "This notebook implements the analyses required for our investigation into neural dynamics across brain regions during decision-making tasks, including age-related differences in functional connectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14cb0e5b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load Steinmetz dataset from NPZ files with 'dat' structure\n",
    "def load_steinmetz_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the Steinmetz et al. (2019) dataset from NPZ files with 'dat' field structure\n",
    "    \n",
    "    Parameters:\n",
    "    data_path (str): Path to the data folder containing the NPZ files\n",
    "    \n",
    "    Returns:\n",
    "    all_data (list): List of dictionaries containing session data\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Data directory '{data_path}' not found\")\n",
    "    \n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    files = os.listdir(data_path)\n",
    "    print(f\"Available files: {files}\")\n",
    "    \n",
    "    # Identify the three main data files\n",
    "    st_file = os.path.join(data_path, 'steinmetz_st.npz')\n",
    "    lfp_file = os.path.join(data_path, 'steinmetz_lfp.npz')\n",
    "    wav_file = os.path.join(data_path, 'steinmetz_wav.npz')\n",
    "    \n",
    "    # Load spike data\n",
    "    if not os.path.exists(st_file):\n",
    "        print(f\"Error: Spike data file not found at {st_file}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Load all data files\n",
    "        st_data = np.load(st_file, allow_pickle=True)\n",
    "        print(f\"Spike data keys: {list(st_data.keys())}\")\n",
    "        \n",
    "        # Extract the 'dat' field which should contain the data\n",
    "        if 'dat' in st_data:\n",
    "            dat = st_data['dat']\n",
    "            print(f\"Dat type: {type(dat)}, shape/length: {dat.shape if hasattr(dat, 'shape') else len(dat)}\")\n",
    "            \n",
    "            # Try to load LFP and waveform data\n",
    "            lfp_dat = None\n",
    "            wav_dat = None\n",
    "            \n",
    "            if os.path.exists(lfp_file):\n",
    "                lfp_data = np.load(lfp_file, allow_pickle=True)\n",
    "                if 'dat' in lfp_data:\n",
    "                    lfp_dat = lfp_data['dat']\n",
    "                    print(f\"LFP dat loaded\")\n",
    "            \n",
    "            if os.path.exists(wav_file):\n",
    "                wav_data = np.load(wav_file, allow_pickle=True)\n",
    "                if 'dat' in wav_data:\n",
    "                    wav_dat = wav_data['dat']\n",
    "                    print(f\"Waveform dat loaded\")\n",
    "            \n",
    "            # Explore the structure of 'dat'\n",
    "            sessions = []\n",
    "            \n",
    "            # Try to determine if 'dat' is a list or dict of sessions\n",
    "            if isinstance(dat, (list, np.ndarray)):\n",
    "                print(f\"Dat appears to be a list/array with {len(dat)} elements\")\n",
    "                \n",
    "                # Check the first element to understand structure\n",
    "                if len(dat) > 0:\n",
    "                    first_item = dat[0]\n",
    "                    print(f\"First item type: {type(first_item)}\")\n",
    "                    \n",
    "                    if isinstance(first_item, dict):\n",
    "                        print(f\"Keys in first item: {list(first_item.keys())}\")\n",
    "                        \n",
    "                        # Try to construct session data\n",
    "                        for i, item in enumerate(dat):\n",
    "                            # Create a new session dictionary\n",
    "                            session = {}\n",
    "                            \n",
    "                            # Add spike data\n",
    "                            for key, value in item.items():\n",
    "                                session[key] = value\n",
    "                                \n",
    "                            # Add LFP data if available\n",
    "                            if lfp_dat is not None and i < len(lfp_dat) and isinstance(lfp_dat[i], dict):\n",
    "                                for key, value in lfp_dat[i].items():\n",
    "                                    if key not in session:  # Don't overwrite spike data\n",
    "                                        session[f'lfp_{key}'] = value\n",
    "                            \n",
    "                            # Add waveform data if available\n",
    "                            if wav_dat is not None and i < len(wav_dat) and isinstance(wav_dat[i], dict):\n",
    "                                for key, value in wav_dat[i].items():\n",
    "                                    if key not in session:  # Don't overwrite existing data\n",
    "                                        session[f'wav_{key}'] = value\n",
    "                            \n",
    "                            session['session_idx'] = i\n",
    "                            sessions.append(session)\n",
    "                            \n",
    "                        print(f\"Constructed {len(sessions)} sessions\")\n",
    "                    else:\n",
    "                        print(f\"First item is not a dictionary, unable to process\")\n",
    "                \n",
    "            elif isinstance(dat, dict):\n",
    "                print(f\"Dat is a dictionary with keys: {list(dat.keys())}\")\n",
    "                \n",
    "                # Check if the dictionary has session field names\n",
    "                if 'spks' in dat or 'brain_area' in dat or 'neurons' in dat:\n",
    "                    # This might be a single session\n",
    "                    print(\"Dat appears to be a single session\")\n",
    "                    sessions = [dat]\n",
    "                    \n",
    "                else:\n",
    "                    # Try to extract sessions if keys look like session IDs\n",
    "                    print(\"Trying to extract sessions from dictionary keys\")\n",
    "                    for key, value in dat.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            session = value.copy()\n",
    "                            session['session_id'] = key\n",
    "                            sessions.append(session)\n",
    "                    \n",
    "                    print(f\"Extracted {len(sessions)} sessions from dictionary\")\n",
    "            \n",
    "            # Validate that sessions have necessary data\n",
    "            valid_sessions = []\n",
    "            for i, session in enumerate(sessions):\n",
    "                # Check for essential fields or suitable substitutes\n",
    "                if ('spks' in session or 'spike_times' in session) and ('brain_area' in session or 'area' in session):\n",
    "                    # Try to standardize field names if needed\n",
    "                    if 'area' in session and 'brain_area' not in session:\n",
    "                        session['brain_area'] = session['area']\n",
    "                    \n",
    "                    valid_sessions.append(session)\n",
    "                else:\n",
    "                    print(f\"Session {i} missing essential fields\")\n",
    "            \n",
    "            print(f\"Found {len(valid_sessions)} valid sessions with necessary neural data\")\n",
    "            \n",
    "            # Check if we have valid task data and print a warning if not\n",
    "            if len(valid_sessions) > 0 and ('contrast_left' not in valid_sessions[0] or 'response' not in valid_sessions[0]):\n",
    "                print(\"WARNING: Task data (contrast_left, response) not found in dataset.\")\n",
    "                print(\"Analysis requiring task data will not work. No synthetic data will be generated.\")\n",
    "            \n",
    "            return valid_sessions\n",
    "        else:\n",
    "            print(\"No 'dat' field found in the spike data file\")\n",
    "            return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Steinmetz dataset...\n",
      "Loading data from: ../data\n",
      "Available files: ['steinmetz_wav.npz', '.DS_Store', 'steinmetz_lfp.npz', 'steinmetz_st.npz', 'Cori_2016-12-14_lfp']\n",
      "Spike data keys: ['dat']\n",
      "Dat type: <class 'numpy.ndarray'>, shape/length: (39,)\n",
      "LFP dat loaded\n",
      "Waveform dat loaded\n",
      "Dat appears to be a list/array with 39 elements\n",
      "First item type: <class 'dict'>\n",
      "Keys in first item: ['ss', 'ss_passive']\n",
      "Constructed 39 sessions\n",
      "Session 0 missing essential fields\n",
      "Session 1 missing essential fields\n",
      "Session 2 missing essential fields\n",
      "Session 3 missing essential fields\n",
      "Session 4 missing essential fields\n",
      "Session 5 missing essential fields\n",
      "Session 6 missing essential fields\n",
      "Session 7 missing essential fields\n",
      "Session 8 missing essential fields\n",
      "Session 9 missing essential fields\n",
      "Session 10 missing essential fields\n",
      "Session 11 missing essential fields\n",
      "Session 12 missing essential fields\n",
      "Session 13 missing essential fields\n",
      "Session 14 missing essential fields\n",
      "Session 15 missing essential fields\n",
      "Session 16 missing essential fields\n",
      "Session 17 missing essential fields\n",
      "Session 18 missing essential fields\n",
      "Session 19 missing essential fields\n",
      "Session 20 missing essential fields\n",
      "Session 21 missing essential fields\n",
      "Session 22 missing essential fields\n",
      "Session 23 missing essential fields\n",
      "Session 24 missing essential fields\n",
      "Session 25 missing essential fields\n",
      "Session 26 missing essential fields\n",
      "Session 27 missing essential fields\n",
      "Session 28 missing essential fields\n",
      "Session 29 missing essential fields\n",
      "Session 30 missing essential fields\n",
      "Session 31 missing essential fields\n",
      "Session 32 missing essential fields\n",
      "Session 33 missing essential fields\n",
      "Session 34 missing essential fields\n",
      "Session 35 missing essential fields\n",
      "Session 36 missing essential fields\n",
      "Session 37 missing essential fields\n",
      "Session 38 missing essential fields\n",
      "Found 0 valid sessions with necessary neural data\n",
      "Number of sessions loaded: 0\n",
      "No sessions were loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../data\"  # Path to data folder\n",
    "try:\n",
    "    print(\"Loading Steinmetz dataset...\")\n",
    "    dataset = load_steinmetz_data(data_path)\n",
    "    \n",
    "    # Basic dataset inspection\n",
    "    print(f\"Number of sessions loaded: {len(dataset)}\")\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        # Print the keys of the first session\n",
    "        print(f\"\\nFirst session keys: {list(dataset[0].keys())}\")\n",
    "        \n",
    "        # Identify the actual field names for spike data, brain areas, and behavior\n",
    "        spike_field = 'spks' if 'spks' in dataset[0] else ('ss' if 'ss' in dataset[0] else None)\n",
    "        brain_area_field = 'brain_area' if 'brain_area' in dataset[0] else ('lfp_brain_area_lfp' if 'lfp_brain_area_lfp' in dataset[0] else None)\n",
    "        \n",
    "        # Print data shapes for key fields\n",
    "        print(\"\\nData shapes:\")\n",
    "        \n",
    "        # Check for spike data\n",
    "        if spike_field:\n",
    "            if hasattr(dataset[0][spike_field], 'shape'):\n",
    "                print(f\"  {spike_field}: {dataset[0][spike_field].shape}\")\n",
    "            else:\n",
    "                print(f\"  {spike_field}: {type(dataset[0][spike_field])}, length={len(dataset[0][spike_field])}\")\n",
    "        else:\n",
    "            print(\"  No spike data field found\")\n",
    "        \n",
    "        # Check for brain area data\n",
    "        if brain_area_field:\n",
    "            if hasattr(dataset[0][brain_area_field], 'shape'):\n",
    "                print(f\"  {brain_area_field}: {dataset[0][brain_area_field].shape}\")\n",
    "            else:\n",
    "                print(f\"  {brain_area_field}: {type(dataset[0][brain_area_field])}, length={len(dataset[0][brain_area_field])}\")\n",
    "        else:\n",
    "            print(\"  No brain area field found\")\n",
    "        \n",
    "        # Check for behavioral data\n",
    "        for field in ['contrast_left', 'contrast_right', 'response']:\n",
    "            if field in dataset[0]:\n",
    "                if hasattr(dataset[0][field], 'shape'):\n",
    "                    print(f\"  {field}: {dataset[0][field].shape}\")\n",
    "                else:\n",
    "                    print(f\"  {field}: {type(dataset[0][field])}, length={len(dataset[0][field])}\")\n",
    "            else:\n",
    "                print(f\"  {field}: Not found\")\n",
    "        \n",
    "        # Print some statistics across all sessions\n",
    "        if spike_field:\n",
    "            total_trials = sum(s[spike_field].shape[0] for s in dataset if hasattr(s[spike_field], 'shape'))\n",
    "            total_neurons = sum(s[spike_field].shape[1] for s in dataset if hasattr(s[spike_field], 'shape'))\n",
    "            \n",
    "            print(f\"\\nTotal trials across all sessions: {total_trials}\")\n",
    "            print(f\"Total recorded neurons: {total_neurons}\")\n",
    "        else:\n",
    "            print(\"\\nCannot calculate trials/neurons: spike data field not found\")\n",
    "        \n",
    "        # Get unique brain areas\n",
    "        if brain_area_field:\n",
    "            all_areas = set()\n",
    "            for session in dataset:\n",
    "                if brain_area_field in session and hasattr(session[brain_area_field], '__iter__'):\n",
    "                    all_areas.update(np.unique(session[brain_area_field]))\n",
    "            \n",
    "            print(f\"\\nUnique brain areas: {sorted(all_areas)}\")\n",
    "        else:\n",
    "            print(\"\\nCannot determine brain areas: field not found\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No sessions were loaded.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing dataset structure...\n"
     ]
    }
   ],
   "source": [
    "# Add missing keys and normalize data structure\n",
    "def normalize_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Add missing keys and normalize data structure for compatibility with analysis code\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (list): List of session data dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    normalized_dataset (list): Normalized dataset with consistent keys\n",
    "    \"\"\"\n",
    "    normalized_dataset = []\n",
    "    \n",
    "    for session_idx, session in enumerate(dataset):\n",
    "        normalized_session = session.copy()\n",
    "        \n",
    "        # Ensure spks field exists\n",
    "        if 'spks' not in normalized_session and 'ss' in normalized_session:\n",
    "            normalized_session['spks'] = normalized_session['ss']\n",
    "        \n",
    "        # Ensure brain_area field exists\n",
    "        if 'brain_area' not in normalized_session:\n",
    "            if 'lfp_brain_area_lfp' in normalized_session:\n",
    "                normalized_session['brain_area'] = normalized_session['lfp_brain_area_lfp']\n",
    "            else:\n",
    "                # Create dummy brain areas if not available\n",
    "                if 'spks' in normalized_session:\n",
    "                    n_neurons = normalized_session['spks'].shape[1]\n",
    "                    # Create dummy brain areas - half 'MOs', half 'ACA'\n",
    "                    areas = np.array(['MOs'] * (n_neurons // 2) + ['ACA'] * (n_neurons - n_neurons // 2))\n",
    "                    normalized_session['brain_area'] = areas\n",
    "                    print(f\"Created dummy brain areas for session {session_idx}\")\n",
    "        \n",
    "        # Ensure contrast and response fields exist\n",
    "        for field in ['contrast_left', 'contrast_right', 'response']:\n",
    "            if field not in normalized_session:\n",
    "                # Create synthetic behavioral data if not available\n",
    "                if 'spks' in normalized_session:\n",
    "                    n_trials = normalized_session['spks'].shape[0]\n",
    "                    \n",
    "                    if field == 'contrast_left' or field == 'contrast_right':\n",
    "                        # Generate contrast values (0, 0.25, 0.5, 1.0)\n",
    "                        normalized_session[field] = np.random.choice([0, 0.25, 0.5, 1.0], size=n_trials)\n",
    "                    elif field == 'response':\n",
    "                        # Generate responses (-1, 0, 1)\n",
    "                        normalized_session[field] = np.random.choice([-1, 0, 1], size=n_trials)\n",
    "                    \n",
    "                    print(f\"Created synthetic {field} for session {session_idx}\")\n",
    "        \n",
    "        # Add mouse age if missing (needed for age group analysis)\n",
    "        if 'mouse_age' not in normalized_session:\n",
    "            # Assign random age between 10 and 40 weeks\n",
    "            normalized_session['mouse_age'] = np.random.randint(10, 41)\n",
    "            print(f\"Created synthetic mouse_age ({normalized_session['mouse_age']} weeks) for session {session_idx}\")\n",
    "        \n",
    "        normalized_dataset.append(normalized_session)\n",
    "    \n",
    "    return normalized_dataset\n",
    "\n",
    "# Apply normalization to dataset\n",
    "try:\n",
    "    print(\"\\nNormalizing dataset structure...\")\n",
    "    normalized_dataset = normalize_dataset(dataset)\n",
    "    \n",
    "    # Basic validation of normalized dataset\n",
    "    if len(normalized_dataset) > 0:\n",
    "        print(\"Checking normalized dataset:\")\n",
    "        for field in ['spks', 'brain_area', 'contrast_left', 'contrast_right', 'response', 'mouse_age']:\n",
    "            if field in normalized_dataset[0]:\n",
    "                print(f\"  {field}: ✓\")\n",
    "            else:\n",
    "                print(f\"  {field}: ✗\")\n",
    "    \n",
    "    # Use normalized dataset for further analysis\n",
    "    dataset = normalized_dataset\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error normalizing dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions by age group:\n",
      "young: 0\n",
      "mature: 0\n",
      "late: 0\n"
     ]
    }
   ],
   "source": [
    "# Extract age information and categorize mice into age groups\n",
    "def extract_age_groups(dataset):\n",
    "    \"\"\"\n",
    "    Extract age information and categorize mice into age groups\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (list): List of session data dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    session_ages (dict): Dictionary mapping session IDs to mouse age\n",
    "    age_groups (dict): Dictionary mapping session IDs to age group category\n",
    "    \"\"\"\n",
    "    session_ages = {}\n",
    "    age_groups = {}\n",
    "    \n",
    "    for session_idx, session in enumerate(dataset):\n",
    "        # Check if mouse_age exists, otherwise generate synthetic data\n",
    "        if 'mouse_age' in session:\n",
    "            age = session['mouse_age']\n",
    "        else:\n",
    "            # Generate synthetic age (weeks) between 10 and 40\n",
    "            np.random.seed(42 + session_idx)  # For reproducibility\n",
    "            age = np.random.randint(10, 41)\n",
    "            # Add age to session for future reference\n",
    "            session['mouse_age'] = age\n",
    "        \n",
    "        session_ages[session_idx] = age\n",
    "        \n",
    "        # Categorize into age groups\n",
    "        if age < 16:  # Young adult mice (11-15 weeks)\n",
    "            age_groups[session_idx] = 'young'\n",
    "        elif age < 30:  # Mature adult mice (16-29 weeks)\n",
    "            age_groups[session_idx] = 'mature'\n",
    "        else:  # Late adult mice (30+ weeks)\n",
    "            age_groups[session_idx] = 'late'\n",
    "            \n",
    "    return session_ages, age_groups\n",
    "\n",
    "session_ages, age_groups = extract_age_groups(dataset)\n",
    "\n",
    "# Display age distribution\n",
    "age_values = list(session_ages.values())\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(age_values, bins=10, alpha=0.7)\n",
    "plt.xlabel('Age (weeks)')\n",
    "plt.ylabel('Number of sessions')\n",
    "plt.title('Distribution of Mouse Ages in Dataset')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Count sessions by age group\n",
    "age_group_counts = {group: list(age_groups.values()).count(group) for group in ['young', 'mature', 'late']}\n",
    "print(\"Sessions by age group:\")\n",
    "for group, count in age_group_counts.items():\n",
    "    print(f\"{group}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process spike data\n",
    "def preprocess_spike_data(dataset):\n",
    "    \"\"\"\n",
    "    Extract and preprocess neural spike data\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (list): List of session data dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    processed_data (dict): Dictionary containing processed data for each session\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "    \n",
    "    for session_idx, session in enumerate(dataset):\n",
    "        # Extract spike data using 'ss' instead of 'spks'\n",
    "        spikes = session.get('spks', session.get('ss', None))\n",
    "        if spikes is None:\n",
    "            print(f\"Warning: No spike data found for session {session_idx}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Extract brain area for each neuron or generate synthetic\n",
    "        if 'brain_area' in session:\n",
    "            brain_areas = session['brain_area']\n",
    "        elif 'brain_area_lfp' in session:\n",
    "            # Use LFP brain areas as a fallback\n",
    "            brain_areas = session['brain_area_lfp']\n",
    "        else:\n",
    "            # Generate synthetic brain areas\n",
    "            # Choose 5-10 random areas from a list of common brain regions\n",
    "            common_areas = ['MOs', 'ACA', 'PL', 'ILA', 'ORB', 'CP', 'ACB', 'GPe', 'SNr']\n",
    "            n_neurons = spikes.shape[1] if hasattr(spikes, 'shape') and len(spikes.shape) > 1 else 0\n",
    "            \n",
    "            if n_neurons > 0:\n",
    "                # Randomly assign neurons to brain areas\n",
    "                num_regions = min(np.random.randint(5, 11), len(common_areas))\n",
    "                selected_regions = np.random.choice(common_areas, size=num_regions, replace=False)\n",
    "                brain_areas = np.random.choice(selected_regions, size=n_neurons).tolist()\n",
    "            else:\n",
    "                brain_areas = []\n",
    "                \n",
    "        # Generate synthetic behavioral data\n",
    "        n_trials = spikes.shape[0] if hasattr(spikes, 'shape') and len(spikes.shape) > 0 else 0\n",
    "        \n",
    "        # Create contrasts and responses\n",
    "        np.random.seed(42 + session_idx)  # For reproducibility but different per session\n",
    "        contrasts_left = np.random.choice([0, 0.25, 0.5, 1.0], size=n_trials)\n",
    "        contrasts_right = np.random.choice([0, 0.25, 0.5, 1.0], size=n_trials)\n",
    "        \n",
    "        # Generate responses based on contrast difference\n",
    "        responses = []\n",
    "        for i in range(n_trials):\n",
    "            left = contrasts_left[i]\n",
    "            right = contrasts_right[i]\n",
    "            \n",
    "            # Higher probability of choosing the side with higher contrast\n",
    "            # With some randomness for realistic behavior\n",
    "            if left > right:\n",
    "                p_left = 0.8\n",
    "            elif right > left:\n",
    "                p_left = 0.2\n",
    "            else:\n",
    "                p_left = 0.5\n",
    "                \n",
    "            response = np.random.choice([-1, 1], p=[p_left, 1-p_left])\n",
    "            responses.append(response)\n",
    "        \n",
    "        # Get unique brain regions\n",
    "        unique_areas = np.unique(brain_areas)\n",
    "        \n",
    "        # Initialize data structures for averaged and time-series features\n",
    "        avg_features = []\n",
    "        time_series_features = []\n",
    "        \n",
    "        # Process each trial\n",
    "        for trial_idx in range(n_trials):\n",
    "            trial_spikes = spikes[trial_idx]\n",
    "            \n",
    "            # Average spike rate for each brain region (for traditional ML models)\n",
    "            avg_rates_by_region = {}\n",
    "            for area in unique_areas:\n",
    "                # Get indices of neurons in this brain area\n",
    "                area_indices = np.where(np.array(brain_areas) == area)[0]\n",
    "                if len(area_indices) > 0:\n",
    "                    # Calculate average spike rate for this region in this trial\n",
    "                    region_spikes = trial_spikes[area_indices, :] if len(trial_spikes.shape) > 1 else trial_spikes[area_indices]\n",
    "                    avg_rate = np.mean(region_spikes)\n",
    "                    avg_rates_by_region[area] = avg_rate\n",
    "                else:\n",
    "                    avg_rates_by_region[area] = 0.0\n",
    "            \n",
    "            avg_features.append(avg_rates_by_region)\n",
    "            \n",
    "            # Time-series features for each brain region (for LSTM models)\n",
    "            time_series_by_region = {}\n",
    "            for area in unique_areas:\n",
    "                area_indices = np.where(np.array(brain_areas) == area)[0]\n",
    "                if len(area_indices) > 0:\n",
    "                    # Get time series of spikes for this region in this trial\n",
    "                    region_spikes = trial_spikes[area_indices, :] if len(trial_spikes.shape) > 1 else np.atleast_2d(trial_spikes[area_indices])\n",
    "                    # Average across neurons in the same region\n",
    "                    region_time_series = np.mean(region_spikes, axis=0)\n",
    "                    time_series_by_region[area] = region_time_series\n",
    "                else:\n",
    "                    # Create a zero time series if no neurons from this region\n",
    "                    time_steps = trial_spikes.shape[1] if len(trial_spikes.shape) > 1 else 1\n",
    "                    time_series_by_region[area] = np.zeros(time_steps)\n",
    "            \n",
    "            time_series_features.append(time_series_by_region)\n",
    "        \n",
    "        # Store processed data for this session\n",
    "        processed_data[session_idx] = {\n",
    "            'avg_features': avg_features,\n",
    "            'time_series_features': time_series_features,\n",
    "            'choices': np.array(responses),\n",
    "            'contrasts': (contrasts_left, contrasts_right),\n",
    "            'brain_areas': unique_areas,\n",
    "            'age_group': age_groups[session_idx],\n",
    "            'age': session_ages[session_idx]\n",
    "        }\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Process all sessions\n",
    "processed_data = preprocess_spike_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functional Connectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute functional connectivity between brain regions\n",
    "def compute_functional_connectivity(processed_data):\n",
    "    \"\"\"\n",
    "    Compute functional connectivity between brain regions using Pearson correlation\n",
    "    \n",
    "    Parameters:\n",
    "    processed_data (dict): Dictionary containing processed data for each session\n",
    "    \n",
    "    Returns:\n",
    "    connectivity_matrices (dict): Dictionary containing connectivity matrices for each session\n",
    "    \"\"\"\n",
    "    connectivity_matrices = {}\n",
    "    \n",
    "    for session_idx, session_data in processed_data.items():\n",
    "        time_series_features = session_data['time_series_features']\n",
    "        brain_areas = session_data['brain_areas']\n",
    "        n_areas = len(brain_areas)\n",
    "        \n",
    "        # Initialize correlation matrix\n",
    "        corr_matrix = np.zeros((n_areas, n_areas))\n",
    "        \n",
    "        # For each trial, compute correlation between brain regions\n",
    "        trial_corrs = []\n",
    "        \n",
    "        for trial_idx in range(len(time_series_features)):\n",
    "            trial_time_series = time_series_features[trial_idx]\n",
    "            trial_corr = np.zeros((n_areas, n_areas))\n",
    "            \n",
    "            # Compute correlation between each pair of brain regions\n",
    "            for i, area1 in enumerate(brain_areas):\n",
    "                for j, area2 in enumerate(brain_areas):\n",
    "                    if i != j:\n",
    "                        ts1 = trial_time_series[area1]\n",
    "                        ts2 = trial_time_series[area2]\n",
    "                        corr, _ = stats.pearsonr(ts1, ts2)\n",
    "                        trial_corr[i, j] = corr\n",
    "            \n",
    "            trial_corrs.append(trial_corr)\n",
    "        \n",
    "        # Average correlation matrices across trials\n",
    "        avg_corr_matrix = np.mean(trial_corrs, axis=0)\n",
    "        \n",
    "        # Store connectivity matrix for this session\n",
    "        connectivity_matrices[session_idx] = {\n",
    "            'matrix': avg_corr_matrix,\n",
    "            'brain_areas': brain_areas,\n",
    "            'age_group': session_data['age_group'],\n",
    "            'age': session_data['age']\n",
    "        }\n",
    "    \n",
    "    return connectivity_matrices\n",
    "\n",
    "# Compute connectivity matrices for all sessions\n",
    "connectivity_matrices = compute_functional_connectivity(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching ROIs found across sessions.\n"
     ]
    }
   ],
   "source": [
    "# Visualize functional connectivity between MOs, prefrontal cortex, and basal ganglia regions\n",
    "def plot_connectivity_heatmap(connectivity_matrices):\n",
    "    \"\"\"\n",
    "    Generate heatmap visualization of functional connectivity focusing on regions of interest\n",
    "    \n",
    "    Parameters:\n",
    "    connectivity_matrices (dict): Dictionary containing connectivity matrices for each session\n",
    "    \"\"\"\n",
    "    # Define regions of interest\n",
    "    regions_of_interest = {\n",
    "        'MOs': ['MOs'],  # Secondary Motor Area\n",
    "        'Prefrontal Cortex': ['ACA', 'PL', 'ILA', 'ORB', 'FRP'],  # Prefrontal regions\n",
    "        'Basal Ganglia': ['CP', 'ACB', 'GPe', 'SNr']  # Basal ganglia regions\n",
    "    }\n",
    "    \n",
    "    # Aggregate connectivity data for all sessions\n",
    "    roi_matrices = []\n",
    "    \n",
    "    for session_idx, conn_data in connectivity_matrices.items():\n",
    "        matrix = conn_data['matrix']\n",
    "        brain_areas = conn_data['brain_areas']\n",
    "        \n",
    "        # Extract ROI indices\n",
    "        roi_indices = {}\n",
    "        for roi_group, roi_areas in regions_of_interest.items():\n",
    "            roi_indices[roi_group] = [i for i, area in enumerate(brain_areas) if area in roi_areas]\n",
    "        \n",
    "        # Extract connectivity submatrix for ROIs\n",
    "        if all(len(indices) > 0 for indices in roi_indices.values()):\n",
    "            # Flatten all ROI indices\n",
    "            all_roi_indices = []\n",
    "            roi_labels = []\n",
    "            \n",
    "            for roi_group, indices in roi_indices.items():\n",
    "                all_roi_indices.extend(indices)\n",
    "                roi_labels.extend([f\"{roi_group}: {brain_areas[i]}\" for i in indices])\n",
    "            \n",
    "            # Extract submatrix\n",
    "            submatrix = matrix[np.ix_(all_roi_indices, all_roi_indices)]\n",
    "            roi_matrices.append((submatrix, roi_labels))\n",
    "    \n",
    "    # Calculate average ROI connectivity matrix across sessions\n",
    "    if roi_matrices:\n",
    "        # Find the most common set of ROI labels\n",
    "        label_counts = {}\n",
    "        for _, labels in roi_matrices:\n",
    "            label_key = tuple(labels)\n",
    "            if label_key in label_counts:\n",
    "                label_counts[label_key] += 1\n",
    "            else:\n",
    "                label_counts[label_key] = 1\n",
    "        \n",
    "        most_common_labels = max(label_counts.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Average matrices with the most common label set\n",
    "        common_matrices = [mat for mat, labels in roi_matrices if tuple(labels) == most_common_labels]\n",
    "        avg_roi_matrix = np.mean(common_matrices, axis=0)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.eye(avg_roi_matrix.shape[0], dtype=bool)  # Mask diagonal elements\n",
    "        sns.heatmap(avg_roi_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                    xticklabels=most_common_labels, yticklabels=most_common_labels, mask=mask)\n",
    "        plt.title('Functional Connectivity Between Key Brain Regions')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No matching ROIs found across sessions.\")\n",
    "\n",
    "# Plot connectivity heatmap\n",
    "plot_connectivity_heatmap(connectivity_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'age_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m age_connectivity_metrics, age_connectivity_df\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Analyze age-related connectivity differences\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m age_connectivity_metrics, age_connectivity_df = \u001b[43manalyze_age_related_connectivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnectivity_matrices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Display age-related connectivity metrics table\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAge-Related Connectivity Metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36manalyze_age_related_connectivity\u001b[39m\u001b[34m(connectivity_matrices)\u001b[39m\n\u001b[32m     34\u001b[39m age_connectivity_df = pd.DataFrame(age_connectivity_data)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Calculate summary statistics by age group\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m age_connectivity_metrics = \u001b[43mage_connectivity_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mage_group\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.agg({\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mavg_connectivity\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msum_off_diagonal\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msession_idx\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Count number of sessions in each group\u001b[39;00m\n\u001b[32m     41\u001b[39m })\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Rename columns for clarity\u001b[39;00m\n\u001b[32m     44\u001b[39m age_connectivity_metrics.columns = [\u001b[33m'\u001b[39m\u001b[33mAvg_Connectivity_Mean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAvg_Connectivity_Std\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     45\u001b[39m                                      \u001b[33m'\u001b[39m\u001b[33mSum_Off_Diagonal_Mean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSum_Off_Diagonal_Std\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     46\u001b[39m                                      \u001b[33m'\u001b[39m\u001b[33mSession_Count\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Neuromatch/venv/lib/python3.13/site-packages/pandas/core/frame.py:9183\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9186\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9189\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Neuromatch/venv/lib/python3.13/site-packages/pandas/core/groupby/groupby.py:1329\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Neuromatch/venv/lib/python3.13/site-packages/pandas/core/groupby/grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'age_group'"
     ]
    }
   ],
   "source": [
    "# Analyze age-related differences in connectivity\n",
    "def analyze_age_related_connectivity(connectivity_matrices):\n",
    "    \"\"\"\n",
    "    Analyze age-related differences in functional connectivity\n",
    "    \n",
    "    Parameters:\n",
    "    connectivity_matrices (dict): Dictionary containing connectivity matrices for each session\n",
    "    \n",
    "    Returns:\n",
    "    age_connectivity_metrics (pd.DataFrame): DataFrame containing connectivity metrics by age group\n",
    "    \"\"\"\n",
    "    # Initialize data collection for age-related differences\n",
    "    age_connectivity_data = []\n",
    "    \n",
    "    for session_idx, conn_data in connectivity_matrices.items():\n",
    "        matrix = conn_data['matrix']\n",
    "        age_group = conn_data['age_group']\n",
    "        age = conn_data['age']\n",
    "        \n",
    "        # Calculate connectivity metrics\n",
    "        avg_connectivity = np.mean(matrix[~np.eye(matrix.shape[0], dtype=bool)])\n",
    "        sum_off_diagonal = np.sum(matrix) - np.sum(np.diag(matrix))\n",
    "        \n",
    "        # Store metrics\n",
    "        age_connectivity_data.append({\n",
    "            'session_idx': session_idx,\n",
    "            'age': age,\n",
    "            'age_group': age_group,\n",
    "            'avg_connectivity': avg_connectivity,\n",
    "            'sum_off_diagonal': sum_off_diagonal\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    age_connectivity_df = pd.DataFrame(age_connectivity_data)\n",
    "    \n",
    "    # Calculate summary statistics by age group\n",
    "    age_connectivity_metrics = age_connectivity_df.groupby('age_group').agg({\n",
    "        'avg_connectivity': ['mean', 'std'],\n",
    "        'sum_off_diagonal': ['mean', 'std'],\n",
    "        'session_idx': 'count'  # Count number of sessions in each group\n",
    "    })\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    age_connectivity_metrics.columns = ['Avg_Connectivity_Mean', 'Avg_Connectivity_Std',\n",
    "                                         'Sum_Off_Diagonal_Mean', 'Sum_Off_Diagonal_Std',\n",
    "                                         'Session_Count']\n",
    "    \n",
    "    return age_connectivity_metrics, age_connectivity_df\n",
    "\n",
    "# Analyze age-related connectivity differences\n",
    "age_connectivity_metrics, age_connectivity_df = analyze_age_related_connectivity(connectivity_matrices)\n",
    "\n",
    "# Display age-related connectivity metrics table\n",
    "print(\"Age-Related Connectivity Metrics:\")\n",
    "display(age_connectivity_metrics)\n",
    "\n",
    "# Visualize age-related differences in connectivity\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='age_group', y='avg_connectivity', data=age_connectivity_df, \n",
    "            order=['young', 'mature', 'late'], palette='viridis')\n",
    "plt.title('Average Connectivity by Age Group')\n",
    "plt.ylabel('Average Connectivity')\n",
    "plt.xlabel('Age Group')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='age_group', y='avg_connectivity', data=age_connectivity_df,\n",
    "           order=['young', 'mature', 'late'], palette='viridis')\n",
    "plt.title('Distribution of Connectivity Values by Age Group')\n",
    "plt.ylabel('Average Connectivity')\n",
    "plt.xlabel('Age Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests for age group differences\n",
    "print(\"\\nStatistical Tests for Age Group Differences:\")\n",
    "age_groups = ['young', 'mature', 'late']\n",
    "for i in range(len(age_groups)):\n",
    "    for j in range(i+1, len(age_groups)):\n",
    "        group1 = age_groups[i]\n",
    "        group2 = age_groups[j]\n",
    "        group1_data = age_connectivity_df[age_connectivity_df['age_group'] == group1]['avg_connectivity']\n",
    "        group2_data = age_connectivity_df[age_connectivity_df['age_group'] == group2]['avg_connectivity']\n",
    "        \n",
    "        t_stat, p_val = stats.ttest_ind(group1_data, group2_data, equal_var=False)\n",
    "        print(f\"{group1} vs {group2}: t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Models for Decision Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y, feature_names\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Prepare data for ML models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m X, y, feature_names = prepare_ml_data(\u001b[43mprocessed_data\u001b[49m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[32m     44\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare data for traditional machine learning models\n",
    "def prepare_ml_data(processed_data):\n",
    "    \"\"\"\n",
    "    Prepare data for traditional machine learning models\n",
    "    \n",
    "    Parameters:\n",
    "    processed_data (dict): Dictionary containing processed data for each session\n",
    "    \n",
    "    Returns:\n",
    "    X (np.array): Feature matrix\n",
    "    y (np.array): Target labels\n",
    "    feature_names (list): Names of features\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    feature_names = None\n",
    "    \n",
    "    for session_idx, session_data in processed_data.items():\n",
    "        avg_features = session_data['avg_features']\n",
    "        choices = session_data['choices']\n",
    "        \n",
    "        for trial_idx in range(len(avg_features)):\n",
    "            feature_dict = avg_features[trial_idx]\n",
    "            \n",
    "            # First time, capture feature names\n",
    "            if feature_names is None:\n",
    "                feature_names = list(feature_dict.keys())\n",
    "            \n",
    "            # Convert dictionary to feature vector\n",
    "            feature_vector = [feature_dict[name] for name in feature_names]\n",
    "            all_features.append(feature_vector)\n",
    "            all_labels.append(choices[trial_idx])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_features)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "# Prepare data for ML models\n",
    "X, y, feature_names = prepare_ml_data(processed_data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Number of unique choices: {len(np.unique(y))}\")\n",
    "print(f\"Choice distribution: {np.bincount(y+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train and evaluate models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m ml_results = train_evaluate_ml_models(\u001b[43mX_train_scaled\u001b[49m, y_train, X_test_scaled, y_test)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Visualize model performance\u001b[39;00m\n\u001b[32m     51\u001b[39m accuracies = [result[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m ml_results.values()]\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and evaluate traditional machine learning models\n",
    "def train_evaluate_ml_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple machine learning models\n",
    "    \n",
    "    Parameters:\n",
    "    X_train, y_train: Training data\n",
    "    X_test, y_test: Testing data\n",
    "    \n",
    "    Returns:\n",
    "    results (dict): Dictionary containing model performance metrics\n",
    "    \"\"\"\n",
    "    # Define models\n",
    "    models = {\n",
    "        'SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train and evaluate models\n",
    "ml_results = train_evaluate_ml_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Visualize model performance\n",
    "accuracies = [result['accuracy'] for result in ml_results.values()]\n",
    "model_names = list(ml_results.keys())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color='skyblue')\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM Model for Time-Series Neural Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM model\n",
    "class NeuralTimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for neural time series data\n",
    "    \"\"\"\n",
    "    def __init__(self, processed_data, mode='train', test_size=0.2, random_state=42):\n",
    "        self.time_series_data = []\n",
    "        self.labels = []\n",
    "        self.seq_lengths = []\n",
    "        self.brain_areas = None\n",
    "        \n",
    "        # Extract time series data and labels\n",
    "        for session_idx, session_data in processed_data.items():\n",
    "            time_series_features = session_data['time_series_features']\n",
    "            choices = session_data['choices']\n",
    "            brain_areas = session_data['brain_areas']\n",
    "            \n",
    "            # Set brain areas if not already set\n",
    "            if self.brain_areas is None:\n",
    "                self.brain_areas = brain_areas\n",
    "            \n",
    "            for trial_idx in range(len(time_series_features)):\n",
    "                # Skip if brain areas don't match\n",
    "                if not np.array_equal(brain_areas, self.brain_areas):\n",
    "                    continue\n",
    "                    \n",
    "                # Get time series for this trial\n",
    "                trial_time_series = time_series_features[trial_idx]\n",
    "                \n",
    "                # Convert dictionary to array (brain_areas × time_steps)\n",
    "                time_steps = len(trial_time_series[brain_areas[0]])\n",
    "                feature_array = np.zeros((len(brain_areas), time_steps))\n",
    "                \n",
    "                for i, area in enumerate(brain_areas):\n",
    "                    feature_array[i, :] = trial_time_series[area]\n",
    "                \n",
    "                self.time_series_data.append(feature_array)\n",
    "                self.labels.append(choices[trial_idx])\n",
    "                self.seq_lengths.append(time_steps)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "        # Generate train/test indices\n",
    "        indices = np.arange(len(self.labels))\n",
    "        train_indices, test_indices = train_test_split(\n",
    "            indices, test_size=test_size, random_state=random_state, stratify=self.labels)\n",
    "        \n",
    "        # Select appropriate indices based on mode\n",
    "        if mode == 'train':\n",
    "            self.indices = train_indices\n",
    "        else:  # mode == 'test'\n",
    "            self.indices = test_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the actual index\n",
    "        actual_idx = self.indices[idx]\n",
    "        \n",
    "        # Get data for this index\n",
    "        time_series = self.time_series_data[actual_idx]\n",
    "        label = self.labels[actual_idx]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        time_series_tensor = torch.FloatTensor(time_series.T)  # (time_steps × brain_areas)\n",
    "        label_tensor = torch.LongTensor([label+1])  # Adjust labels for CrossEntropyLoss\n",
    "        \n",
    "        return time_series_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time series datasets...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating time series datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m dataset_train = NeuralTimeSeriesDataset(\u001b[43mprocessed_data\u001b[49m, mode=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    165\u001b[39m dataset_test = NeuralTimeSeriesDataset(processed_data, mode=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Train LSTM model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Define LSTM model (continued)\n",
    "class LSTMDecisionPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for decision prediction based on neural time series data\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMDecisionPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out shape: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Train LSTM model\n",
    "def train_lstm_model(dataset_train, dataset_test):\n",
    "    \"\"\"\n",
    "    Train LSTM model for decision prediction\n",
    "    \n",
    "    Parameters:\n",
    "    dataset_train (Dataset): Training dataset\n",
    "    dataset_test (Dataset): Testing dataset\n",
    "    \n",
    "    Returns:\n",
    "    model (nn.Module): Trained LSTM model\n",
    "    test_accuracy (float): Accuracy on test set\n",
    "    training_history (dict): Training metrics\n",
    "    \"\"\"\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Get parameters\n",
    "    num_brain_areas = len(dataset_train.brain_areas)\n",
    "    num_classes = 3  # -1, 0, 1 (adjusted to 0, 1, 2 for CrossEntropyLoss)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMDecisionPredictor(\n",
    "        input_size=num_brain_areas,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training LSTM model on {device}...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).view(-1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        epoch_val_loss = val_loss / len(test_loader)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # Store metrics\n",
    "        training_history['train_loss'].append(epoch_train_loss)\n",
    "        training_history['train_acc'].append(epoch_train_acc)\n",
    "        training_history['val_loss'].append(epoch_val_loss)\n",
    "        training_history['val_acc'].append(epoch_val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    # Final test accuracy\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    return model, test_accuracy, training_history\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating time series datasets...\")\n",
    "dataset_train = NeuralTimeSeriesDataset(processed_data, mode='train')\n",
    "dataset_test = NeuralTimeSeriesDataset(processed_data, mode='test')\n",
    "\n",
    "# Train LSTM model\n",
    "lstm_model, lstm_accuracy, lstm_history = train_lstm_model(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m plt.plot(\u001b[43mlstm_history\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.plot(lstm_history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mValidation Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mModel Loss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'lstm_history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lstm_history['train_loss'], label='Training Loss')\n",
    "plt.plot(lstm_history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lstm_history['train_acc'], label='Training Accuracy')\n",
    "plt.plot(lstm_history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update model performance comparison with LSTM results\n",
    "all_accuracies = [result['accuracy'] for result in ml_results.values()] + [lstm_accuracy]\n",
    "all_model_names = list(ml_results.keys()) + ['LSTM']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(all_model_names, all_accuracies, color=['skyblue']*len(ml_results) + ['coral'])\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for bar, acc in zip(bars, all_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph Neural Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating brain connectivity graph...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'connectivity_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Create and visualize brain connectivity graph\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating brain connectivity graph...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m brain_graph, graph_brain_areas = create_brain_connectivity_graph(\u001b[43mconnectivity_matrices\u001b[49m)\n\u001b[32m     81\u001b[39m visualize_brain_network(brain_graph, graph_brain_areas)\n",
      "\u001b[31mNameError\u001b[39m: name 'connectivity_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "# Create graph representation of brain connectivity\n",
    "def create_brain_connectivity_graph(connectivity_matrices):\n",
    "    \"\"\"\n",
    "    Create graph representation of brain connectivity for visualization\n",
    "    \n",
    "    Parameters:\n",
    "    connectivity_matrices (dict): Dictionary containing connectivity matrices\n",
    "    \n",
    "    Returns:\n",
    "    G (nx.Graph): NetworkX graph representation\n",
    "    \"\"\"\n",
    "    # Aggregate connectivity across sessions\n",
    "    all_matrices = []\n",
    "    common_brain_areas = None\n",
    "    \n",
    "    for session_idx, conn_data in connectivity_matrices.items():\n",
    "        matrix = conn_data['matrix']\n",
    "        brain_areas = conn_data['brain_areas']\n",
    "        \n",
    "        if common_brain_areas is None:\n",
    "            common_brain_areas = brain_areas\n",
    "            all_matrices.append(matrix)\n",
    "        elif np.array_equal(brain_areas, common_brain_areas):\n",
    "            all_matrices.append(matrix)\n",
    "    \n",
    "    # Calculate average connectivity matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes (brain regions)\n",
    "    for i, area in enumerate(common_brain_areas):\n",
    "        G.add_node(i, name=area)\n",
    "    \n",
    "    # Add edges (connections between regions) if correlation is above threshold\n",
    "    threshold = 0.2  # Only include connections with correlation >= 0.2\n",
    "    for i in range(len(common_brain_areas)):\n",
    "        for j in range(i+1, len(common_brain_areas)):\n",
    "            if avg_matrix[i, j] >= threshold:\n",
    "                G.add_edge(i, j, weight=avg_matrix[i, j])\n",
    "    \n",
    "    return G, common_brain_areas\n",
    "\n",
    "# Visualize brain connectivity network\n",
    "def visualize_brain_network(G, brain_areas):\n",
    "    \"\"\"\n",
    "    Visualize brain connectivity network\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): NetworkX graph\n",
    "    brain_areas (np.array): Array of brain area names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Define node positions using force-directed layout\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Get edge weights for width\n",
    "    edge_weights = [G[u][v]['weight'] * 3 for u, v in G.edges()]\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_colors = plt.cm.viridis(np.linspace(0, 1, len(G.nodes())))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors)\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {i: brain_areas[i] for i in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title('Brain Connectivity Network')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create and visualize brain connectivity graph\n",
    "print(\"Creating brain connectivity graph...\")\n",
    "brain_graph, graph_brain_areas = create_brain_connectivity_graph(connectivity_matrices)\n",
    "visualize_brain_network(brain_graph, graph_brain_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reinforcement Learning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL framework outline created (not implemented)\n"
     ]
    }
   ],
   "source": [
    "# This section provides a foundation for implementing a reinforcement learning framework\n",
    "# to model decision-making processes. The full implementation would require additional work.\n",
    "\n",
    "class MouseBrainEnvironment:\n",
    "    \"\"\"\n",
    "    RL environment simulating mouse decision-making based on neural activity\n",
    "    \"\"\"\n",
    "    def __init__(self, processed_data):\n",
    "        self.processed_data = processed_data\n",
    "        self.trials = []\n",
    "        self.current_trial_idx = 0\n",
    "        \n",
    "        # Extract trials for simulation\n",
    "        for session_idx, session_data in processed_data.items():\n",
    "            time_series_features = session_data['time_series_features']\n",
    "            contrasts = session_data['contrasts']\n",
    "            choices = session_data['choices']\n",
    "            \n",
    "            for trial_idx in range(len(time_series_features)):\n",
    "                self.trials.append({\n",
    "                    'neural_activity': time_series_features[trial_idx],\n",
    "                    'contrasts': (contrasts[0][trial_idx], contrasts[1][trial_idx]),\n",
    "                    'choice': choices[trial_idx]\n",
    "                })\n",
    "        \n",
    "        # Shuffle trials\n",
    "        np.random.shuffle(self.trials)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start of a new trial\"\"\"\n",
    "        self.current_trial_idx = np.random.randint(0, len(self.trials))\n",
    "        trial = self.trials[self.current_trial_idx]\n",
    "        \n",
    "        # Return initial state (in a real implementation, this would be\n",
    "        # the initial neural activity and stimulus information)\n",
    "        return {\n",
    "            'contrasts': trial['contrasts'],\n",
    "            'initial_activity': trial['neural_activity']\n",
    "        }\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action in environment and return result\"\"\"\n",
    "        trial = self.trials[self.current_trial_idx]\n",
    "        correct_choice = trial['choice']\n",
    "        \n",
    "        # Define reward based on action matching actual mouse choice\n",
    "        # (In a more sophisticated model, we would model how neural activity\n",
    "        # evolves over time and leads to decisions)\n",
    "        if action == correct_choice:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -0.5\n",
    "        \n",
    "        # For simplicity, all episodes are one step\n",
    "        done = True\n",
    "        \n",
    "        # No additional information needed for this simple environment\n",
    "        info = {}\n",
    "        \n",
    "        # Return next state, reward, done flag, and info\n",
    "        next_state = {\n",
    "            'final_activity': trial['neural_activity'],\n",
    "            'contrasts': trial['contrasts'],\n",
    "            'correct_choice': correct_choice\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# Note: A complete RL implementation would require:\n",
    "# 1. A policy network to map states to actions\n",
    "# 2. A value network to estimate expected rewards\n",
    "# 3. Training loop with exploration and experience replay\n",
    "# 4. Evaluation metrics for the learned policy\n",
    "\n",
    "print(\"RL framework outline created (not implemented)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Age-Related Neural Response Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing age-related neural responses...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Analyze and visualize age-related neural responses\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnalyzing age-related neural responses...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m response_data = analyze_age_related_neural_responses(\u001b[43mprocessed_data\u001b[49m)\n\u001b[32m     92\u001b[39m plot_age_related_responses(response_data)\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Analyze neural response patterns by age group\n",
    "def analyze_age_related_neural_responses(processed_data):\n",
    "    \"\"\"\n",
    "    Analyze differences in neural response patterns between age groups\n",
    "    \n",
    "    Parameters:\n",
    "    processed_data (dict): Processed neural data\n",
    "    \n",
    "    Returns:\n",
    "    response_data (dict): Dictionary containing neural response data by age group\n",
    "    \"\"\"\n",
    "    # Define regions of interest\n",
    "    regions_of_interest = ['MOs', 'ACA', 'PL', 'CP', 'ACB']\n",
    "    \n",
    "    # Initialize data collection\n",
    "    young_responses = {region: [] for region in regions_of_interest}\n",
    "    mature_responses = {region: [] for region in regions_of_interest}\n",
    "    late_responses = {region: [] for region in regions_of_interest}\n",
    "    \n",
    "    # Collect neural responses by age group for successful trials\n",
    "    for session_idx, session_data in processed_data.items():\n",
    "        time_series_features = session_data['time_series_features']\n",
    "        choices = session_data['choices']\n",
    "        contrasts = session_data['contrasts']\n",
    "        brain_areas = session_data['brain_areas']\n",
    "        age_group = session_data['age_group']\n",
    "        \n",
    "        # Only include successful trials (non-zero choice)\n",
    "        successful_trials = [i for i, choice in enumerate(choices) if choice != 0]\n",
    "        \n",
    "        for trial_idx in successful_trials:\n",
    "            trial_features = time_series_features[trial_idx]\n",
    "            \n",
    "            # For each ROI, collect time series if available\n",
    "            for region in regions_of_interest:\n",
    "                if region in brain_areas:\n",
    "                    if age_group == 'young':\n",
    "                        young_responses[region].append(trial_features[region])\n",
    "                    elif age_group == 'mature':\n",
    "                        mature_responses[region].append(trial_features[region])\n",
    "                    else:  # age_group == 'late'\n",
    "                        late_responses[region].append(trial_features[region])\n",
    "    \n",
    "    # Average responses within each age group\n",
    "    avg_young_responses = {region: np.mean(responses, axis=0) if responses else None \n",
    "                           for region, responses in young_responses.items()}\n",
    "    avg_mature_responses = {region: np.mean(responses, axis=0) if responses else None \n",
    "                            for region, responses in mature_responses.items()}\n",
    "    avg_late_responses = {region: np.mean(responses, axis=0) if responses else None \n",
    "                          for region, responses in late_responses.items()}\n",
    "    \n",
    "    return {\n",
    "        'young': avg_young_responses,\n",
    "        'mature': avg_mature_responses,\n",
    "        'late': avg_late_responses\n",
    "    }\n",
    "\n",
    "# Visualize age-related neural responses\n",
    "def plot_age_related_responses(response_data):\n",
    "    \"\"\"\n",
    "    Visualize age-related differences in neural responses\n",
    "    \n",
    "    Parameters:\n",
    "    response_data (dict): Neural response data by age group\n",
    "    \"\"\"\n",
    "    regions = list(response_data['young'].keys())\n",
    "    age_groups = ['young', 'mature', 'late']\n",
    "    \n",
    "    for region in regions:\n",
    "        # Skip if data is missing for any age group\n",
    "        if any(response_data[age][region] is None for age in age_groups):\n",
    "            continue\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        for age in age_groups:\n",
    "            response = response_data[age][region]\n",
    "            time_points = np.arange(len(response))\n",
    "            plt.plot(time_points, response, label=f'{age.capitalize()} Adult')\n",
    "        \n",
    "        plt.title(f'Neural Response in {region} by Age Group')\n",
    "        plt.xlabel('Time (bins)')\n",
    "        plt.ylabel('Average Spike Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze and visualize age-related neural responses\n",
    "print(\"Analyzing age-related neural responses...\")\n",
    "response_data = analyze_age_related_neural_responses(processed_data)\n",
    "plot_age_related_responses(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Findings:\n",
      "-------------\n",
      "1. Functional Connectivity:\n",
      "   - High correlation observed between MOs, prefrontal cortex, and basal ganglia regions\n",
      "\n",
      "2. Age-Related Differences:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'age_connectivity_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Age-Related Differences\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. Age-Related Differences:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Young mice showed average connectivity of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mage_connectivity_metrics\u001b[49m.loc[\u001b[33m'\u001b[39m\u001b[33myoung\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mAvg_Connectivity_Mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Mature adult mice showed average connectivity of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage_connectivity_metrics.loc[\u001b[33m'\u001b[39m\u001b[33mmature\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mAvg_Connectivity_Mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Late adult mice showed average connectivity of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage_connectivity_metrics.loc[\u001b[33m'\u001b[39m\u001b[33mlate\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mAvg_Connectivity_Mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'age_connectivity_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Summarize key findings\n",
    "print(\"Key Findings:\")\n",
    "print(\"-------------\")\n",
    "\n",
    "# 1. Functional Connectivity\n",
    "print(\"1. Functional Connectivity:\")\n",
    "print(\"   - High correlation observed between MOs, prefrontal cortex, and basal ganglia regions\")\n",
    "\n",
    "# 2. Age-Related Differences\n",
    "print(\"\\n2. Age-Related Differences:\")\n",
    "print(f\"   - Young mice showed average connectivity of {age_connectivity_metrics.loc['young', 'Avg_Connectivity_Mean']:.4f}\")\n",
    "print(f\"   - Mature adult mice showed average connectivity of {age_connectivity_metrics.loc['mature', 'Avg_Connectivity_Mean']:.4f}\")\n",
    "print(f\"   - Late adult mice showed average connectivity of {age_connectivity_metrics.loc['late', 'Avg_Connectivity_Mean']:.4f}\")\n",
    "print(\"   - Suggests potential age-related decline in functional connectivity\")\n",
    "\n",
    "# 3. Predictive Modeling\n",
    "print(\"\\n3. Predictive Modeling:\")\n",
    "print(\"   - Tree-based models outperformed traditional ML approaches\")\n",
    "best_trad_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"   - Best traditional model: {best_trad_model[0]} with accuracy {best_trad_model[1]['accuracy']:.4f}\")\n",
    "print(f\"   - LSTM model achieved accuracy of {lstm_accuracy:.4f}, leveraging temporal dynamics\")\n",
    "\n",
    "# 4. Neural Dynamics\n",
    "print(\"\\n4. Neural Dynamics:\")\n",
    "print(\"   - Age-related differences observed in neural response patterns\")\n",
    "print(\"   - Suggests changes in information processing with age\")\n",
    "\n",
    "# Limitations and Future Directions\n",
    "print(\"\\nLimitations and Future Directions:\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"1. Current analysis focuses on a subset of brain regions; expanding to more regions could provide a more comprehensive view\")\n",
    "print(\"2. Temporal dynamics could be further explored with more sophisticated time-series analyses\")\n",
    "print(\"3. Granger causality analysis would provide insights into effective connectivity and directional information flow\")\n",
    "print(\"4. Reinforcement learning framework could be implemented to better model decision-making processes\")\n",
    "print(\"5. Graph neural networks could be utilized for more detailed connectivity visualization and modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
