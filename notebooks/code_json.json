{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Neural Navigators: Decoding Neural Circuits for Decision Making\n",
       "\n",
       "This notebook implements the analyses required for our investigation into neural dynamics across brain regions during decision-making tasks, including age-related differences in functional connectivity."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Data Loading"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from scipy import stats\n",
       "from sklearn.model_selection import train_test_split, cross_val_score\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "from sklearn.svm import SVC\n",
       "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
       "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "import torch_geometric\n",
       "import networkx as nx\n",
       "import pickle\n",
       "import os\n",
       "from tqdm import tqdm\n",
       "\n",
       "# Set random seeds for reproducibility\n",
       "np.random.seed(42)\n",
       "torch.manual_seed(42)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Function to load Steinmetz dataset\n",
       "def load_steinmetz_data(data_path):\n",
       "    \"\"\"\n",
       "    Load the Steinmetz et al. (2019) dataset\n",
       "    \n",
       "    Parameters:\n",
       "    data_path (str): Path to the dataset files\n",
       "    \n",
       "    Returns:\n",
       "    all_data (list): List of dictionaries containing session data\n",
       "    \"\"\"\n",
       "    all_data = []\n",
       "    for file in os.listdir(data_path):\n",
       "        if file.endswith('.pkl'):\n",
       "            with open(os.path.join(data_path, file), 'rb') as f:\n",
       "                session_data = pickle.load(f)\n",
       "                all_data.append(session_data)\n",
       "    return all_data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load the dataset\n",
       "data_path = \"path/to/steinmetz_data\"  # Update with actual path\n",
       "dataset = load_steinmetz_data(data_path)\n",
       "\n",
       "# Basic dataset inspection\n",
       "print(f\"Number of sessions: {len(dataset)}\")\n",
       "print(f\"Example session keys: {list(dataset[0].keys())}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Data Preprocessing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extract age information and categorize mice into age groups\n",
       "def extract_age_groups(dataset):\n",
       "    \"\"\"\n",
       "    Extract age information and categorize mice into age groups\n",
       "    \n",
       "    Parameters:\n",
       "    dataset (list): List of session data dictionaries\n",
       "    \n",
       "    Returns:\n",
       "    session_ages (dict): Dictionary mapping session IDs to mouse age\n",
       "    age_groups (dict): Dictionary mapping session IDs to age group category\n",
       "    \"\"\"\n",
       "    session_ages = {}\n",
       "    age_groups = {}\n",
       "    \n",
       "    for session_idx, session in enumerate(dataset):\n",
       "        # Extract mouse age (weeks)\n",
       "        age = session.get('mouse_age', None)  # Adjust field name based on actual data structure\n",
       "        session_ages[session_idx] = age\n",
       "        \n",
       "        # Categorize into age groups\n",
       "        if age < 16:  # Young adult mice (11-15 weeks)\n",
       "            age_groups[session_idx] = 'young'\n",
       "        elif age < 30:  # Mature adult mice (16-29 weeks)\n",
       "            age_groups[session_idx] = 'mature'\n",
       "        else:  # Late adult mice (30+ weeks)\n",
       "            age_groups[session_idx] = 'late'\n",
       "            \n",
       "    return session_ages, age_groups\n",
       "\n",
       "session_ages, age_groups = extract_age_groups(dataset)\n",
       "\n",
       "# Display age distribution\n",
       "age_values = list(session_ages.values())\n",
       "plt.figure(figsize=(10, 6))\n",
       "plt.hist(age_values, bins=10, alpha=0.7)\n",
       "plt.xlabel('Age (weeks)')\n",
       "plt.ylabel('Number of sessions')\n",
       "plt.title('Distribution of Mouse Ages in Dataset')\n",
       "plt.grid(alpha=0.3)\n",
       "plt.show()\n",
       "\n",
       "# Count sessions by age group\n",
       "age_group_counts = {group: list(age_groups.values()).count(group) for group in ['young', 'mature', 'late']}\n",
       "print(\"Sessions by age group:\")\n",
       "for group, count in age_group_counts.items():\n",
       "    print(f\"{group}: {count}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extract and process spike data\n",
       "def preprocess_spike_data(dataset):\n",
       "    \"\"\"\n",
       "    Extract and preprocess neural spike data\n",
       "    \n",
       "    Parameters:\n",
       "    dataset (list): List of session data dictionaries\n",
       "    \n",
       "    Returns:\n",
       "    processed_data (dict): Dictionary containing processed data for each session\n",
       "    \"\"\"\n",
       "    processed_data = {}\n",
       "    \n",
       "    for session_idx, session in enumerate(dataset):\n",
       "        # Extract relevant data\n",
       "        spikes = session['spks']  # Spike data\n",
       "        brain_areas = session['brain_area']  # Brain area for each neuron\n",
       "        contrasts = session['contrast_left'], session['contrast_right']  # Visual contrasts\n",
       "        choices = session['response']  # Mouse choices (e.g., -1: left, 1: right, 0: no response)\n",
       "        \n",
       "        # Get unique brain regions\n",
       "        unique_areas = np.unique(brain_areas)\n",
       "        \n",
       "        # Initialize data structures for averaged and time-series features\n",
       "        avg_features = []\n",
       "        time_series_features = []\n",
       "        \n",
       "        # Process each trial\n",
       "        for trial_idx in range(len(spikes)):\n",
       "            trial_spikes = spikes[trial_idx]\n",
       "            \n",
       "            # Average spike rate for each brain region (for traditional ML models)\n",
       "            avg_rates_by_region = {}\n",
       "            for area in unique_areas:\n",
       "                # Get indices of neurons in this brain area\n",
       "                area_indices = np.where(brain_areas == area)[0]\n",
       "                if len(area_indices) > 0:\n",
       "                    # Calculate average spike rate for this region in this trial\n",
       "                    region_spikes = trial_spikes[area_indices, :]\n",
       "                    avg_rate = np.mean(region_spikes)\n",
       "                    avg_rates_by_region[area] = avg_rate\n",
       "                else:\n",
       "                    avg_rates_by_region[area] = 0.0\n",
       "            \n",
       "            avg_features.append(avg_rates_by_region)\n",
       "            \n",
       "            # Time-series features for each brain region (for LSTM models)\n",
       "            time_series_by_region = {}\n",
       "            for area in unique_areas:\n",
       "                area_indices = np.where(brain_areas == area)[0]\n",
       "                if len(area_indices) > 0:\n",
       "                    # Get time series of spikes for this region in this trial\n",
       "                    region_spikes = trial_spikes[area_indices, :]\n",
       "                    # Average across neurons in the same region\n",
       "                    region_time_series = np.mean(region_spikes, axis=0)\n",
       "                    time_series_by_region[area] = region_time_series\n",
       "                else:\n",
       "                    # Create a zero time series if no neurons from this region\n",
       "                    time_series_by_region[area] = np.zeros(trial_spikes.shape[1])\n",
       "            \n",
       "            time_series_features.append(time_series_by_region)\n",
       "        \n",
       "        # Store processed data for this session\n",
       "        processed_data[session_idx] = {\n",
       "            'avg_features': avg_features,\n",
       "            'time_series_features': time_series_features,\n",
       "            'choices': choices,\n",
       "            'contrasts': contrasts,\n",
       "            'brain_areas': unique_areas,\n",
       "            'age_group': age_groups[session_idx],\n",
       "            'age': session_ages[session_idx]\n",
       "        }\n",
       "    \n",
       "    return processed_data\n",
       "\n",
       "# Process all sessions\n",
       "processed_data = preprocess_spike_data(dataset)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Functional Connectivity Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compute functional connectivity between brain regions\n",
       "def compute_functional_connectivity(processed_data):\n",
       "    \"\"\"\n",
       "    Compute functional connectivity between brain regions using Pearson correlation\n",
       "    \n",
       "    Parameters:\n",
       "    processed_data (dict): Dictionary containing processed data for each session\n",
       "    \n",
       "    Returns:\n",
       "    connectivity_matrices (dict): Dictionary containing connectivity matrices for each session\n",
       "    \"\"\"\n",
       "    connectivity_matrices = {}\n",
       "    \n",
       "    for session_idx, session_data in processed_data.items():\n",
       "        time_series_features = session_data['time_series_features']\n",
       "        brain_areas = session_data['brain_areas']\n",
       "        n_areas = len(brain_areas)\n",
       "        \n",
       "        # Initialize correlation matrix\n",
       "        corr_matrix = np.zeros((n_areas, n_areas))\n",
       "        \n",
       "        # For each trial, compute correlation between brain regions\n",
       "        trial_corrs = []\n",
       "        \n",
       "        for trial_idx in range(len(time_series_features)):\n",
       "            trial_time_series = time_series_features[trial_idx]\n",
       "            trial_corr = np.zeros((n_areas, n_areas))\n",
       "            \n",
       "            # Compute correlation between each pair of brain regions\n",
       "            for i, area1 in enumerate(brain_areas):\n",
       "                for j, area2 in enumerate(brain_areas):\n",
       "                    if i != j:\n",
       "                        ts1 = trial_time_series[area1]\n",
       "                        ts2 = trial_time_series[area2]\n",
       "                        corr, _ = stats.pearsonr(ts1, ts2)\n",
       "                        trial_corr[i, j] = corr\n",
       "            \n",
       "            trial_corrs.append(trial_corr)\n",
       "        \n",
       "        # Average correlation matrices across trials\n",
       "        avg_corr_matrix = np.mean(trial_corrs, axis=0)\n",
       "        \n",
       "        # Store connectivity matrix for this session\n",
       "        connectivity_matrices[session_idx] = {\n",
       "            'matrix': avg_corr_matrix,\n",
       "            'brain_areas': brain_areas,\n",
       "            'age_group': session_data['age_group'],\n",
       "            'age': session_data['age']\n",
       "        }\n",
       "    \n",
       "    return connectivity_matrices\n",
       "\n",
       "# Compute connectivity matrices for all sessions\n",
       "connectivity_matrices = compute_functional_connectivity(processed_data)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize functional connectivity between MOs, prefrontal cortex, and basal ganglia regions\n",
       "def plot_connectivity_heatmap(connectivity_matrices):\n",
       "    \"\"\"\n",
       "    Generate heatmap visualization of functional connectivity focusing on regions of interest\n",
       "    \n",
       "    Parameters:\n",
       "    connectivity_matrices (dict): Dictionary containing connectivity matrices for each session\n",
       "    \"\"\"\n",
       "    # Define regions of interest\n",
       "    regions_of_interest = {\n",
       "        'MOs': ['MOs'],  # Secondary Motor Area\n",
       "        'Prefrontal Cortex': ['ACA', 'PL', 'ILA', 'ORB', 'FRP'],  # Prefrontal regions\n",
       "        'Basal Ganglia': ['CP', 'ACB', 'GPe', 'SNr']  # Basal ganglia regions\n",
       "    }\n",
       "    \n",
       "    # Aggregate connectivity data for all sessions\n",
       "    roi_matrices = []\n",
       "    \n",
       "    for session_idx, conn_data in connectivity_matrices.items():\n",
       "        matrix = conn_data['matrix']\n",
       "        brain_areas = conn_data['brain_areas']\n",
       "        \n",
       "        # Extract ROI indices\n",
       "        roi_indices = {}\n",
       "        for roi_group, roi_areas in regions_of_interest.items():\n",
       "            roi_indices[roi_group] = [i for i, area in enumerate(brain_areas) if area in roi_areas]\n",
       "        \n",
       "        # Extract connectivity submatrix for ROIs\n",
       "        if all(len(indices) > 0 for indices in roi_indices.values()):\n",
       "            # Flatten all ROI indices\n",
       "            all_roi_indices = []\n",
       "            roi_labels = []\n",
       "            \n",
       "            for roi_group, indices in roi_indices.items():\n",
       "                all_roi_indices.extend(indices)\n",
       "                roi_labels.extend([f\"{roi_group}: {brain_areas[i]}\" for i in indices])\n",
       "            \n",
       "            # Extract submatrix\n",
       "            submatrix = matrix[np.ix_(all_roi_indices, all_roi_indices)]\n",
       "            roi_matrices.append((submatrix, roi_labels))\n",
       "    \n",
       "    # Calculate average ROI connectivity matrix across sessions\n",
       "    if roi_matrices:\n",
       "        # Find the most common set of ROI labels\n",
       "        label_counts = {}\n",
       "        for _, labels in roi_matrices:\n",
       "            label_key = tuple(labels)\n",
       "            if label_key in label_counts:\n",
       "                label_counts[label_key] += 1\n",
       "            else:\n",
       "                label_counts[label_key] = 1\n",
       "        \n",
       "        most_common_labels = max(label_counts.items(), key=lambda x: x[1])[0]\n",
       "        \n",
       "        # Average matrices with the most common label set\n",
       "        common_matrices = [mat for mat, labels in roi_matrices if tuple(labels) == most_common_labels]\n",
       "        avg_roi_matrix = np.mean(common_matrices, axis=0)\n",
       "        \n",
       "        # Create heatmap\n",
       "        plt.figure(figsize=(12, 10))\n",
       "        mask = np.eye(avg_roi_matrix.shape[0], dtype=bool)  # Mask diagonal elements\n",
       "        sns.heatmap(avg_roi_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
       "                    xticklabels=most_common_labels, yticklabels=most_common_labels, mask=mask)\n",
       "        plt.title('Functional Connectivity Between Key Brain Regions')\n",
       "        plt.xticks(rotation=90)\n",
       "        plt.yticks(rotation=0)\n",
       "        plt.tight_layout()\n",
       "        plt.show()\n",
       "    else:\n",
       "        print(\"No matching ROIs found across sessions.\")\n",
       "\n",
       "# Plot connectivity heatmap\n",
       "plot_connectivity_heatmap(connectivity_matrices)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Analyze age-related differences in connectivity\n",
       "def analyze_age_related_connectivity(connectivity_matrices):\n",
       "    \"\"\"\n",
       "    Analyze age-related differences in functional connectivity\n",
       "    \n",
       "    Parameters:\n",
       "    connectivity_matrices (dict): Dictionary containing connectivity matrices for each session\n",
       "    \n",
       "    Returns:\n",
       "    age_connectivity_metrics (pd.DataFrame): DataFrame containing connectivity metrics by age group\n",
       "    \"\"\"\n",
       "    # Initialize data collection for age-related differences\n",
       "    age_connectivity_data = []\n",
       "    \n",
       "    for session_idx, conn_data in connectivity_matrices.items():\n",
       "        matrix = conn_data['matrix']\n",
       "        age_group = conn_data['age_group']\n",
       "        age = conn_data['age']\n",
       "        \n",
       "        # Calculate connectivity metrics\n",
       "        avg_connectivity = np.mean(matrix[~np.eye(matrix.shape[0], dtype=bool)])\n",
       "        sum_off_diagonal = np.sum(matrix) - np.sum(np.diag(matrix))\n",
       "        \n",
       "        # Store metrics\n",
       "        age_connectivity_data.append({\n",
       "            'session_idx': session_idx,\n",
       "            'age': age,\n",
       "            'age_group': age_group,\n",
       "            'avg_connectivity': avg_connectivity,\n",
       "            'sum_off_diagonal': sum_off_diagonal\n",
       "        })\n",
       "    \n",
       "    # Convert to DataFrame\n",
       "    age_connectivity_df = pd.DataFrame(age_connectivity_data)\n",
       "    \n",
       "    # Calculate summary statistics by age group\n",
       "    age_connectivity_metrics = age_connectivity_df.groupby('age_group').agg({\n",
       "        'avg_connectivity': ['mean', 'std'],\n",
       "        'sum_off_diagonal': ['mean', 'std'],\n",
       "        'session_idx': 'count'  # Count number of sessions in each group\n",
       "    })\n",
       "    \n",
       "    # Rename columns for clarity\n",
       "    age_connectivity_metrics.columns = ['Avg_Connectivity_Mean', 'Avg_Connectivity_Std',\n",
       "                                         'Sum_Off_Diagonal_Mean', 'Sum_Off_Diagonal_Std',\n",
       "                                         'Session_Count']\n",
       "    \n",
       "    return age_connectivity_metrics, age_connectivity_df\n",
       "\n",
       "# Analyze age-related connectivity differences\n",
       "age_connectivity_metrics, age_connectivity_df = analyze_age_related_connectivity(connectivity_matrices)\n",
       "\n",
       "# Display age-related connectivity metrics table\n",
       "print(\"Age-Related Connectivity Metrics:\")\n",
       "display(age_connectivity_metrics)\n",
       "\n",
       "# Visualize age-related differences in connectivity\n",
       "plt.figure(figsize=(12, 5))\n",
       "\n",
       "plt.subplot(1, 2, 1)\n",
       "sns.barplot(x='age_group', y='avg_connectivity', data=age_connectivity_df, \n",
       "            order=['young', 'mature', 'late'], palette='viridis')\n",
       "plt.title('Average Connectivity by Age Group')\n",
       "plt.ylabel('Average Connectivity')\n",
       "plt.xlabel('Age Group')\n",
       "\n",
       "plt.subplot(1, 2, 2)\n",
       "sns.boxplot(x='age_group', y='avg_connectivity', data=age_connectivity_df,\n",
       "           order=['young', 'mature', 'late'], palette='viridis')\n",
       "plt.title('Distribution of Connectivity Values by Age Group')\n",
       "plt.ylabel('Average Connectivity')\n",
       "plt.xlabel('Age Group')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Statistical tests for age group differences\n",
       "print(\"\\nStatistical Tests for Age Group Differences:\")\n",
       "age_groups = ['young', 'mature', 'late']\n",
       "for i in range(len(age_groups)):\n",
       "    for j in range(i+1, len(age_groups)):\n",
       "        group1 = age_groups[i]\n",
       "        group2 = age_groups[j]\n",
       "        group1_data = age_connectivity_df[age_connectivity_df['age_group'] == group1]['avg_connectivity']\n",
       "        group2_data = age_connectivity_df[age_connectivity_df['age_group'] == group2]['avg_connectivity']\n",
       "        \n",
       "        t_stat, p_val = stats.ttest_ind(group1_data, group2_data, equal_var=False)\n",
       "        print(f\"{group1} vs {group2}: t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Machine Learning Models for Decision Prediction"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare data for traditional machine learning models\n",
       "def prepare_ml_data(processed_data):\n",
       "    \"\"\"\n",
       "    Prepare data for traditional machine learning models\n",
       "    \n",
       "    Parameters:\n",
       "    processed_data (dict): Dictionary containing processed data for each session\n",
       "    \n",
       "    Returns:\n",
       "    X (np.array): Feature matrix\n",
       "    y (np.array): Target labels\n",
       "    feature_names (list): Names of features\n",
       "    \"\"\"\n",
       "    all_features = []\n",
       "    all_labels = []\n",
       "    feature_names = None\n",
       "    \n",
       "    for session_idx, session_data in processed_data.items():\n",
       "        avg_features = session_data['avg_features']\n",
       "        choices = session_data['choices']\n",
       "        \n",
       "        for trial_idx in range(len(avg_features)):\n",
       "            feature_dict = avg_features[trial_idx]\n",
       "            \n",
       "            # First time, capture feature names\n",
       "            if feature_names is None:\n",
       "                feature_names = list(feature_dict.keys())\n",
       "            \n",
       "            # Convert dictionary to feature vector\n",
       "            feature_vector = [feature_dict[name] for name in feature_names]\n",
       "            all_features.append(feature_vector)\n",
       "            all_labels.append(choices[trial_idx])\n",
       "    \n",
       "    # Convert to numpy arrays\n",
       "    X = np.array(all_features)\n",
       "    y = np.array(all_labels)\n",
       "    \n",
       "    return X, y, feature_names\n",
       "\n",
       "# Prepare data for ML models\n",
       "X, y, feature_names = prepare_ml_data(processed_data)\n",
       "\n",
       "# Split data into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Scale features\n",
       "scaler = StandardScaler()\n",
       "X_train_scaled = scaler.fit_transform(X_train)\n",
       "X_test_scaled = scaler.transform(X_test)\n",
       "\n",
       "print(f\"Feature matrix shape: {X.shape}\")\n",
       "print(f\"Target vector shape: {y.shape}\")\n",
       "print(f\"Number of unique choices: {len(np.unique(y))}\")\n",
       "print(f\"Choice distribution: {np.bincount(y+1)}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Train and evaluate traditional machine learning models\n",
       "def train_evaluate_ml_models(X_train, y_train, X_test, y_test):\n",
       "    \"\"\"\n",
       "    Train and evaluate multiple machine learning models\n",
       "    \n",
       "    Parameters:\n",
       "    X_train, y_train: Training data\n",
       "    X_test, y_test: Testing data\n",
       "    \n",
       "    Returns:\n",
       "    results (dict): Dictionary containing model performance metrics\n",
       "    \"\"\"\n",
       "    # Define models\n",
       "    models = {\n",
       "        'SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
       "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
       "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
       "    }\n",
       "    \n",
       "    results = {}\n",
       "    \n",
       "    for name, model in models.items():\n",
       "        print(f\"Training {name}...\")\n",
       "        # Train model\n",
       "        model.fit(X_train, y_train)\n",
       "        \n",
       "        # Predict on test set\n",
       "        y_pred = model.predict(X_test)\n",
       "        \n",
       "        # Calculate metrics\n",
       "        accuracy = accuracy_score(y_test, y_pred)\n",
       "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
       "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
       "        \n",
       "        # Store results\n",
       "        results[name] = {\n",
       "            'accuracy': accuracy,\n",
       "            'confusion_matrix': conf_matrix,\n",
       "            'classification_report': class_report,\n",
       "            'model': model\n",
       "        }\n",
       "        \n",
       "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
       "    \n",
       "    return results\n",
       "\n",
       "# Train and evaluate models\n",
       "ml_results = train_evaluate_ml_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
       "\n",
       "# Visualize model performance\n",
       "accuracies = [result['accuracy'] for result in ml_results.values()]\n",
       "model_names = list(ml_results.keys())\n",
       "\n",
       "plt.figure(figsize=(10, 6))\n",
       "bars = plt.bar(model_names, accuracies, color='skyblue')\n",
       "\n",
       "# Add accuracy values on top of bars\n",
       "for bar, acc in zip(bars, accuracies):\n",
       "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
       "             f'{acc:.4f}', ha='center', va='bottom')\n",
       "\n",
       "plt.title('Comparison of Model Accuracies')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.ylim(0, 1)\n",
       "plt.grid(axis='y', alpha=0.3)\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. LSTM Model for Time-Series Neural Data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare data for LSTM model\n",
       "class NeuralTimeSeriesDataset(Dataset):\n",
       "    \"\"\"\n",
       "    PyTorch Dataset for neural time series data\n",
       "    \"\"\"\n",
       "    def __init__(self, processed_data, mode='train', test_size=0.2, random_state=42):\n",
       "        self.time_series_data = []\n",
       "        self.labels = []\n",
       "        self.seq_lengths = []\n",
       "        self.brain_areas = None\n",
       "        \n",
       "        # Extract time series data and labels\n",
       "        for session_idx, session_data in processed_data.items():\n",
       "            time_series_features = session_data['time_series_features']\n",
       "            choices = session_data['choices']\n",
       "            brain_areas = session_data['brain_areas']\n",
       "            \n",
       "            # Set brain areas if not already set\n",
       "            if self.brain_areas is None:\n",
       "                self.brain_areas = brain_areas\n",
       "            \n",
       "            for trial_idx in range(len(time_series_features)):\n",
       "                # Skip if brain areas don't match\n",
       "                if not np.array_equal(brain_areas, self.brain_areas):\n",
       "                    continue\n",
       "                    \n",
       "                # Get time series for this trial\n",
       "                trial_time_series = time_series_features[trial_idx]\n",
       "                \n",
       "                # Convert dictionary to array (brain_areas × time_steps)\n",
       "                time_steps = len(trial_time_series[brain_areas[0]])\n",
       "                feature_array = np.zeros((len(brain_areas), time_steps))\n",
       "                \n",
       "                for i, area in enumerate(brain_areas):\n",
       "                    feature_array[i, :] = trial_time_series[area]\n",
       "                \n",
       "                self.time_series_data.append(feature_array)\n",
       "                self.labels.append(choices[trial_idx])\n",
       "                self.seq_lengths.append(time_steps)\n",
       "        \n",
       "        # Convert to numpy arrays\n",
       "        self.labels = np.array(self.labels)\n",
       "        \n",
       "        # Generate train/test indices\n",
       "        indices = np.arange(len(self.labels))\n",
       "        train_indices, test_indices = train_test_split(\n",
       "            indices, test_size=test_size, random_state=random_state, stratify=self.labels)\n",
       "        \n",
       "        # Select appropriate indices based on mode\n",
       "        if mode == 'train':\n",
       "            self.indices = train_indices\n",
       "        else:  # mode == 'test'\n",
       "            self.indices = test_indices\n",
       "    \n",
       "    def __len__(self):\n",
       "        return len(self.indices)\n",
       "    \n",
       "    def __getitem__(self, idx):\n",
       "        # Get the actual index\n",
       "        actual_idx = self.indices[idx]\n",
       "        \n",
       "        # Get data for this index\n",
       "        time_series = self.time_series_data[actual_idx]\n",
       "        label = self.labels[actual_idx]\n",
       "        \n",
       "        # Convert to torch tensors\n",
       "        time_series_tensor = torch.FloatTensor(time_series.T)  # (time_steps × brain_areas)\n",
       "        label_tensor = torch.LongTensor([label+1])  # Adjust labels for CrossEntropyLoss\n",
       "        \n",
       "        return time_series_tensor, label_tensor\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define LSTM model (continued)\n",
       "class LSTMDecisionPredictor(nn.Module):\n",
       "    \"\"\"\n",
       "    LSTM model for decision prediction based on neural time series data\n",
       "    \"\"\"\n",
       "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
       "        super(LSTMDecisionPredictor, self).__init__()\n",
       "        self.hidden_size = hidden_size\n",
       "        self.num_layers = num_layers\n",
       "        \n",
       "        # LSTM layers\n",
       "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
       "        \n",
       "        # Fully connected layer\n",
       "        self.fc = nn.Linear(hidden_size, num_classes)\n",
       "        \n",
       "    def forward(self, x):\n",
       "        # Initialize hidden state and cell state\n",
       "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
       "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
       "        \n",
       "        # Forward propagate LSTM\n",
       "        out, _ = self.lstm(x, (h0, c0))  # out shape: (batch_size, seq_length, hidden_size)\n",
       "        \n",
       "        # Decode the hidden state of the last time step\n",
       "        out = self.fc(out[:, -1, :])\n",
       "        \n",
       "        return out\n",
       "\n",
       "# Train LSTM model\n",
       "def train_lstm_model(dataset_train, dataset_test):\n",
       "    \"\"\"\n",
       "    Train LSTM model for decision prediction\n",
       "    \n",
       "    Parameters:\n",
       "    dataset_train (Dataset): Training dataset\n",
       "    dataset_test (Dataset): Testing dataset\n",
       "    \n",
       "    Returns:\n",
       "    model (nn.Module): Trained LSTM model\n",
       "    test_accuracy (float): Accuracy on test set\n",
       "    training_history (dict): Training metrics\n",
       "    \"\"\"\n",
       "    # Create data loaders\n",
       "    train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
       "    test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
       "    \n",
       "    # Get parameters\n",
       "    num_brain_areas = len(dataset_train.brain_areas)\n",
       "    num_classes = 3  # -1, 0, 1 (adjusted to 0, 1, 2 for CrossEntropyLoss)\n",
       "    \n",
       "    # Initialize model\n",
       "    model = LSTMDecisionPredictor(\n",
       "        input_size=num_brain_areas,\n",
       "        hidden_size=128,\n",
       "        num_layers=2,\n",
       "        num_classes=num_classes\n",
       "    )\n",
       "    \n",
       "    # Loss and optimizer\n",
       "    criterion = nn.CrossEntropyLoss()\n",
       "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
       "    \n",
       "    # Training loop\n",
       "    num_epochs = 50\n",
       "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "    model.to(device)\n",
       "    \n",
       "    training_history = {\n",
       "        'train_loss': [],\n",
       "        'train_acc': [],\n",
       "        'val_loss': [],\n",
       "        'val_acc': []\n",
       "    }\n",
       "    \n",
       "    print(f\"Training LSTM model on {device}...\")\n",
       "    \n",
       "    for epoch in range(num_epochs):\n",
       "        model.train()\n",
       "        train_loss = 0.0\n",
       "        train_correct = 0\n",
       "        train_total = 0\n",
       "        \n",
       "        for inputs, labels in train_loader:\n",
       "            inputs = inputs.to(device)\n",
       "            labels = labels.to(device).view(-1)\n",
       "            \n",
       "            # Forward pass\n",
       "            outputs = model(inputs)\n",
       "            loss = criterion(outputs, labels)\n",
       "            \n",
       "            # Backward and optimize\n",
       "            optimizer.zero_grad()\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "            \n",
       "            # Track statistics\n",
       "            train_loss += loss.item()\n",
       "            _, predicted = torch.max(outputs.data, 1)\n",
       "            train_total += labels.size(0)\n",
       "            train_correct += (predicted == labels).sum().item()\n",
       "        \n",
       "        # Calculate training metrics\n",
       "        epoch_train_loss = train_loss / len(train_loader)\n",
       "        epoch_train_acc = train_correct / train_total\n",
       "        \n",
       "        # Validation\n",
       "        model.eval()\n",
       "        val_loss = 0.0\n",
       "        val_correct = 0\n",
       "        val_total = 0\n",
       "        \n",
       "        with torch.no_grad():\n",
       "            for inputs, labels in test_loader:\n",
       "                inputs = inputs.to(device)\n",
       "                labels = labels.to(device).view(-1)\n",
       "                \n",
       "                outputs = model(inputs)\n",
       "                loss = criterion(outputs, labels)\n",
       "                \n",
       "                val_loss += loss.item()\n",
       "                _, predicted = torch.max(outputs.data, 1)\n",
       "                val_total += labels.size(0)\n",
       "                val_correct += (predicted == labels).sum().item()\n",
       "        \n",
       "        # Calculate validation metrics\n",
       "        epoch_val_loss = val_loss / len(test_loader)\n",
       "        epoch_val_acc = val_correct / val_total\n",
       "        \n",
       "        # Store metrics\n",
       "        training_history['train_loss'].append(epoch_train_loss)\n",
       "        training_history['train_acc'].append(epoch_train_acc)\n",
       "        training_history['val_loss'].append(epoch_val_loss)\n",
       "        training_history['val_acc'].append(epoch_val_acc)\n",
       "        \n",
       "        # Print progress\n",
       "        if (epoch + 1) % 5 == 0:\n",
       "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
       "                  f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
       "                  f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
       "    \n",
       "    # Final test accuracy\n",
       "    model.eval()\n",
       "    test_correct = 0\n",
       "    test_total = 0\n",
       "    \n",
       "    with torch.no_grad():\n",
       "        for inputs, labels in test_loader:\n",
       "            inputs = inputs.to(device)\n",
       "            labels = labels.to(device).view(-1)\n",
       "            \n",
       "            outputs = model(inputs)\n",
       "            _, predicted = torch.max(outputs.data, 1)\n",
       "            test_total += labels.size(0)\n",
       "            test_correct += (predicted == labels).sum().item()\n",
       "    \n",
       "    test_accuracy = test_correct / test_total\n",
       "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
       "    \n",
       "    return model, test_accuracy, training_history\n",
       "\n",
       "# Create datasets\n",
       "print(\"Creating time series datasets...\")\n",
       "dataset_train = NeuralTimeSeriesDataset(processed_data, mode='train')\n",
       "dataset_test = NeuralTimeSeriesDataset(processed_data, mode='test')\n",
       "\n",
       "# Train LSTM model\n",
       "lstm_model, lstm_accuracy, lstm_history = train_lstm_model(dataset_train, dataset_test)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot training history\n",
       "plt.figure(figsize=(12, 5))\n",
       "\n",
       "plt.subplot(1, 2, 1)\n",
       "plt.plot(lstm_history['train_loss'], label='Training Loss')\n",
       "plt.plot(lstm_history['val_loss'], label='Validation Loss')\n",
       "plt.title('Model Loss')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Loss')\n",
       "plt.legend()\n",
       "\n",
       "plt.subplot(1, 2, 2)\n",
       "plt.plot(lstm_history['train_acc'], label='Training Accuracy')\n",
       "plt.plot(lstm_history['val_acc'], label='Validation Accuracy')\n",
       "plt.title('Model Accuracy')\n",
       "plt.xlabel('Epoch')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.legend()\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()\n",
       "\n",
       "# Update model performance comparison with LSTM results\n",
       "all_accuracies = [result['accuracy'] for result in ml_results.values()] + [lstm_accuracy]\n",
       "all_model_names = list(ml_results.keys()) + ['LSTM']\n",
       "\n",
       "plt.figure(figsize=(10, 6))\n",
       "bars = plt.bar(all_model_names, all_accuracies, color=['skyblue']*len(ml_results) + ['coral'])\n",
       "\n",
       "# Add accuracy values on top of bars\n",
       "for bar, acc in zip(bars, all_accuracies):\n",
       "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
       "             f'{acc:.4f}', ha='center', va='bottom')\n",
       "\n",
       "plt.title('Comparison of Model Accuracies')\n",
       "plt.ylabel('Accuracy')\n",
       "plt.ylim(0, 1)\n",
       "plt.grid(axis='y', alpha=0.3)\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Graph Neural Network Visualization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create graph representation of brain connectivity\n",
       "def create_brain_connectivity_graph(connectivity_matrices):\n",
       "    \"\"\"\n",
       "    Create graph representation of brain connectivity for visualization\n",
       "    \n",
       "    Parameters:\n",
       "    connectivity_matrices (dict): Dictionary containing connectivity matrices\n",
       "    \n",
       "    Returns:\n",
       "    G (nx.Graph): NetworkX graph representation\n",
       "    \"\"\"\n",
       "    # Aggregate connectivity across sessions\n",
       "    all_matrices = []\n",
       "    common_brain_areas = None\n",
       "    \n",
       "    for session_idx, conn_data in connectivity_matrices.items():\n",
       "        matrix = conn_data['matrix']\n",
       "        brain_areas = conn_data['brain_areas']\n",
       "        \n",
       "        if common_brain_areas is None:\n",
       "            common_brain_areas = brain_areas\n",
       "            all_matrices.append(matrix)\n",
       "        elif np.array_equal(brain_areas, common_brain_areas):\n",
       "            all_matrices.append(matrix)\n",
       "    \n",
       "    # Calculate average connectivity matrix\n",
       "    avg_matrix = np.mean(all_matrices, axis=0)\n",
       "    \n",
       "    # Create graph\n",
       "    G = nx.Graph()\n",
       "    \n",
       "    # Add nodes (brain regions)\n",
       "    for i, area in enumerate(common_brain_areas):\n",
       "        G.add_node(i, name=area)\n",
       "    \n",
       "    # Add edges (connections between regions) if correlation is above threshold\n",
       "    threshold = 0.2  # Only include connections with correlation >= 0.2\n",
       "    for i in range(len(common_brain_areas)):\n",
       "        for j in range(i+1, len(common_brain_areas)):\n",
       "            if avg_matrix[i, j] >= threshold:\n",
       "                G.add_edge(i, j, weight=avg_matrix[i, j])\n",
       "    \n",
       "    return G, common_brain_areas\n",
       "\n",
       "# Visualize brain connectivity network\n",
       "def visualize_brain_network(G, brain_areas):\n",
       "    \"\"\"\n",
       "    Visualize brain connectivity network\n",
       "    \n",
       "    Parameters:\n",
       "    G (nx.Graph): NetworkX graph\n",
       "    brain_areas (np.array): Array of brain area names\n",
       "    \"\"\"\n",
       "    plt.figure(figsize=(12, 10))\n",
       "    \n",
       "    # Define node positions using force-directed layout\n",
       "    pos = nx.spring_layout(G, seed=42)\n",
       "    \n",
       "    # Get edge weights for width\n",
       "    edge_weights = [G[u][v]['weight'] * 3 for u, v in G.edges()]\n",
       "    \n",
       "    # Draw edges\n",
       "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')\n",
       "    \n",
       "    # Draw nodes\n",
       "    node_colors = plt.cm.viridis(np.linspace(0, 1, len(G.nodes())))\n",
       "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors)\n",
       "    \n",
       "    # Draw labels\n",
       "    labels = {i: brain_areas[i] for i in G.nodes()}\n",
       "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, font_weight='bold')\n",
       "    \n",
       "    plt.title('Brain Connectivity Network')\n",
       "    plt.axis('off')\n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "\n",
       "# Create and visualize brain connectivity graph\n",
       "print(\"Creating brain connectivity graph...\")\n",
       "brain_graph, graph_brain_areas = create_brain_connectivity_graph(connectivity_matrices)\n",
       "visualize_brain_network(brain_graph, graph_brain_areas)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Reinforcement Learning Framework"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# This section provides a foundation for implementing a reinforcement learning framework\n",
       "# to model decision-making processes. The full implementation would require additional work.\n",
       "\n",
       "class MouseBrainEnvironment:\n",
       "    \"\"\"\n",
       "    RL environment simulating mouse decision-making based on neural activity\n",
       "    \"\"\"\n",
       "    def __init__(self, processed_data):\n",
       "        self.processed_data = processed_data\n",
       "        self.trials = []\n",
       "        self.current_trial_idx = 0\n",
       "        \n",
       "        # Extract trials for simulation\n",
       "        for session_idx, session_data in processed_data.items():\n",
       "            time_series_features = session_data['time_series_features']\n",
       "            contrasts = session_data['contrasts']\n",
       "            choices = session_data['choices']\n",
       "            \n",
       "            for trial_idx in range(len(time_series_features)):\n",
       "                self.trials.append({\n",
       "                    'neural_activity': time_series_features[trial_idx],\n",
       "                    'contrasts': (contrasts[0][trial_idx], contrasts[1][trial_idx]),\n",
       "                    'choice': choices[trial_idx]\n",
       "                })\n",
       "        \n",
       "        # Shuffle trials\n",
       "        np.random.shuffle(self.trials)\n",
       "        \n",
       "    def reset(self):\n",
       "        \"\"\"Reset environment to start of a new trial\"\"\"\n",
       "        self.current_trial_idx = np.random.randint(0, len(self.trials))\n",
       "        trial = self.trials[self.current_trial_idx]\n",
       "        \n",
       "        # Return initial state (in a real implementation, this would be\n",
       "        # the initial neural activity and stimulus information)\n",
       "        return {\n",
       "            'contrasts': trial['contrasts'],\n",
       "            'initial_activity': trial['neural_activity']\n",
       "        }\n",
       "    \n",
       "    def step(self, action):\n",
       "        \"\"\"Take action in environment and return result\"\"\"\n",
       "        trial = self.trials[self.current_trial_idx]\n",
       "        correct_choice = trial['choice']\n",
       "        \n",
       "        # Define reward based on action matching actual mouse choice\n",
       "        # (In a more sophisticated model, we would model how neural activity\n",
       "        # evolves over time and leads to decisions)\n",
       "        if action == correct_choice:\n",
       "            reward = 1.0\n",
       "        else:\n",
       "            reward = -0.5\n",
       "        \n",
       "        # For simplicity, all episodes are one step\n",
       "        done = True\n",
       "        \n",
       "        # No additional information needed for this simple environment\n",
       "        info = {}\n",
       "        \n",
       "        # Return next state, reward, done flag, and info\n",
       "        next_state = {\n",
       "            'final_activity': trial['neural_activity'],\n",
       "            'contrasts': trial['contrasts'],\n",
       "            'correct_choice': correct_choice\n",
       "        }\n",
       "        \n",
       "        return next_state, reward, done, info\n",
       "\n",
       "# Note: A complete RL implementation would require:\n",
       "# 1. A policy network to map states to actions\n",
       "# 2. A value network to estimate expected rewards\n",
       "# 3. Training loop with exploration and experience replay\n",
       "# 4. Evaluation metrics for the learned policy\n",
       "\n",
       "print(\"RL framework outline created (not implemented)\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Age-Related Neural Response Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Analyze neural response patterns by age group\n",
       "def analyze_age_related_neural_responses(processed_data):\n",
       "    \"\"\"\n",
       "    Analyze differences in neural response patterns between age groups\n",
       "    \n",
       "    Parameters:\n",
       "    processed_data (dict): Processed neural data\n",
       "    \n",
       "    Returns:\n",
       "    response_data (dict): Dictionary containing neural response data by age group\n",
       "    \"\"\"\n",
       "    # Define regions of interest\n",
       "    regions_of_interest = ['MOs', 'ACA', 'PL', 'CP', 'ACB']\n",
       "    \n",
       "    # Initialize data collection\n",
       "    young_responses = {region: [] for region in regions_of_interest}\n",
       "    mature_responses = {region: [] for region in regions_of_interest}\n",
       "    late_responses = {region: [] for region in regions_of_interest}\n",
       "    \n",
       "    # Collect neural responses by age group for successful trials\n",
       "    for session_idx, session_data in processed_data.items():\n",
       "        time_series_features = session_data['time_series_features']\n",
       "        choices = session_data['choices']\n",
       "        contrasts = session_data['contrasts']\n",
       "        brain_areas = session_data['brain_areas']\n",
       "        age_group = session_data['age_group']\n",
       "        \n",
       "        # Only include successful trials (non-zero choice)\n",
       "        successful_trials = [i for i, choice in enumerate(choices) if choice != 0]\n",
       "        \n",
       "        for trial_idx in successful_trials:\n",
       "            trial_features = time_series_features[trial_idx]\n",
       "            \n",
       "            # For each ROI, collect time series if available\n",
       "            for region in regions_of_interest:\n",
       "                if region in brain_areas:\n",
       "                    if age_group == 'young':\n",
       "                        young_responses[region].append(trial_features[region])\n",
       "                    elif age_group == 'mature':\n",
       "                        mature_responses[region].append(trial_features[region])\n",
       "                    else:  # age_group == 'late'\n",
       "                        late_responses[region].append(trial_features[region])\n",
       "    \n",
       "    # Average responses within each age group\n",
       "    avg_young_responses = {region: np.mean(responses, axis=0) if responses else None \n",
       "                           for region, responses in young_responses.items()}\n",
       "    avg_mature_responses = {region: np.mean(responses, axis=0) if responses else None \n",
       "                            for region, responses in mature_responses.items()}\n",
       "    avg_late_responses = {region: np.mean(responses, axis=0) if responses else None \n",
       "                          for region, responses in late_responses.items()}\n",
       "    \n",
       "    return {\n",
       "        'young': avg_young_responses,\n",
       "        'mature': avg_mature_responses,\n",
       "        'late': avg_late_responses\n",
       "    }\n",
       "\n",
       "# Visualize age-related neural responses\n",
       "def plot_age_related_responses(response_data):\n",
       "    \"\"\"\n",
       "    Visualize age-related differences in neural responses\n",
       "    \n",
       "    Parameters:\n",
       "    response_data (dict): Neural response data by age group\n",
       "    \"\"\"\n",
       "    regions = list(response_data['young'].keys())\n",
       "    age_groups = ['young', 'mature', 'late']\n",
       "    \n",
       "    for region in regions:\n",
       "        # Skip if data is missing for any age group\n",
       "        if any(response_data[age][region] is None for age in age_groups):\n",
       "            continue\n",
       "        \n",
       "        plt.figure(figsize=(10, 6))\n",
       "        \n",
       "        for age in age_groups:\n",
       "            response = response_data[age][region]\n",
       "            time_points = np.arange(len(response))\n",
       "            plt.plot(time_points, response, label=f'{age.capitalize()} Adult')\n",
       "        \n",
       "        plt.title(f'Neural Response in {region} by Age Group')\n",
       "        plt.xlabel('Time (bins)')\n",
       "        plt.ylabel('Average Spike Rate')\n",
       "        plt.legend()\n",
       "        plt.grid(alpha=0.3)\n",
       "        plt.tight_layout()\n",
       "        plt.show()\n",
       "\n",
       "# Analyze and visualize age-related neural responses\n",
       "print(\"Analyzing age-related neural responses...\")\n",
       "response_data = analyze_age_related_neural_responses(processed_data)\n",
       "plot_age_related_responses(response_data)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Summary and Conclusions"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Summarize key findings\n",
       "print(\"Key Findings:\")\n",
       "print(\"-------------\")\n",
       "\n",
       "# 1. Functional Connectivity\n",
       "print(\"1. Functional Connectivity:\")\n",
       "print(\"   - High correlation observed between MOs, prefrontal cortex, and basal ganglia regions\")\n",
       "\n",
       "# 2. Age-Related Differences\n",
       "print(\"\\n2. Age-Related Differences:\")\n",
       "print(f\"   - Young mice showed average connectivity of {age_connectivity_metrics.loc['young', 'Avg_Connectivity_Mean']:.4f}\")\n",
       "print(f\"   - Mature adult mice showed average connectivity of {age_connectivity_metrics.loc['mature', 'Avg_Connectivity_Mean']:.4f}\")\n",
       "print(f\"   - Late adult mice showed average connectivity of {age_connectivity_metrics.loc['late', 'Avg_Connectivity_Mean']:.4f}\")\n",
       "print(\"   - Suggests potential age-related decline in functional connectivity\")\n",
       "\n",
       "# 3. Predictive Modeling\n",
       "print(\"\\n3. Predictive Modeling:\")\n",
       "print(\"   - Tree-based models outperformed traditional ML approaches\")\n",
       "best_trad_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
       "print(f\"   - Best traditional model: {best_trad_model[0]} with accuracy {best_trad_model[1]['accuracy']:.4f}\")\n",
       "print(f\"   - LSTM model achieved accuracy of {lstm_accuracy:.4f}, leveraging temporal dynamics\")\n",
       "\n",
       "# 4. Neural Dynamics\n",
       "print(\"\\n4. Neural Dynamics:\")\n",
       "print(\"   - Age-related differences observed in neural response patterns\")\n",
       "print(\"   - Suggests changes in information processing with age\")\n",
       "\n",
       "# Limitations and Future Directions\n",
       "print(\"\\nLimitations and Future Directions:\")\n",
       "print(\"--------------------------------\")\n",
       "print(\"1. Current analysis focuses on a subset of brain regions; expanding to more regions could provide a more comprehensive view\")\n",
       "print(\"2. Temporal dynamics could be further explored with more sophisticated time-series analyses\")\n",
       "print(\"3. Granger causality analysis would provide insights into effective connectivity and directional information flow\")\n",
       "print(\"4. Reinforcement learning framework could be implemented to better model decision-making processes\")\n",
       "print(\"5. Graph neural networks could be utilized for more detailed connectivity visualization and modeling\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}