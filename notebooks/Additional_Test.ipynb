{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steinmetz Data Integration Notebook\n",
    "#### This notebook provides functions for loading and integrating the three Steinmetz datasets:\n",
    "- steinmetz_st.npz (spike times)\n",
    "- steinmetz_lfp.npz (local field potentials)\n",
    "- steinmetz_wav.npz (waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, requests\n",
    "import pandas as pd\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set matplotlib defaults\n",
    "rcParams['figure.figsize'] = [20, 4]\n",
    "rcParams['font.size'] = 15\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Data Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_steinmetz_data(data_dir='.'):\n",
    "    \"\"\"\n",
    "    Download Steinmetz datasets if they don't exist locally\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir : str\n",
    "        Directory where data should be stored\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # Define filenames and URLs\n",
    "    fname = ['steinmetz_st.npz', 'steinmetz_wav.npz', 'steinmetz_lfp.npz']\n",
    "    url = [\n",
    "        \"https://osf.io/4bjns/download\",\n",
    "        \"https://osf.io/ugm9v/download\",\n",
    "        \"https://osf.io/kx3v9/download\"\n",
    "    ]\n",
    "    \n",
    "    # Download each file if it doesn't exist\n",
    "    for j in range(len(url)):\n",
    "        file_path = os.path.join(data_dir, fname[j])\n",
    "        if not os.path.isfile(file_path):\n",
    "            try:\n",
    "                print(f\"Downloading {fname[j]}...\")\n",
    "                r = requests.get(url[j])\n",
    "                if r.status_code == requests.codes.ok:\n",
    "                    with open(file_path, \"wb\") as fid:\n",
    "                        fid.write(r.content)\n",
    "                    print(f\"Successfully downloaded {fname[j]}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {fname[j]}, status code: {r.status_code}\")\n",
    "            except requests.ConnectionError:\n",
    "                print(f\"Connection error while downloading {fname[j]}\")\n",
    "        else:\n",
    "            print(f\"{fname[j]} already exists in {data_dir}\")\n",
    "    \n",
    "    print(\"Download process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Data Loading Function\n",
    "def load_steinmetz_data(data_dir='.', verbose=True):\n",
    "    \"\"\"\n",
    "    Load all three Steinmetz datasets\n",
    "    \n",
    "    Parameters:\n",
    "    data_dir : str\n",
    "        Directory where data is stored\n",
    "    verbose : bool\n",
    "        Whether to print detailed information\n",
    "        \n",
    "    Returns:\n",
    "    st_data : numpy.ndarray\n",
    "        Spike times data\n",
    "    lfp_data : numpy.ndarray\n",
    "        LFP data\n",
    "    wav_data : numpy.ndarray\n",
    "        Waveform data\n",
    "    \"\"\"\n",
    "    # Check if files exist, download if needed\n",
    "    for fname in ['steinmetz_st.npz', 'steinmetz_wav.npz', 'steinmetz_lfp.npz']:\n",
    "        if not os.path.isfile(os.path.join(data_dir, fname)):\n",
    "            print(f\"File {fname} not found. Downloading data...\")\n",
    "            download_steinmetz_data(data_dir)\n",
    "            break\n",
    "    \n",
    "    # Load datasets\n",
    "    if verbose:\n",
    "        print(\"Loading datasets...\")\n",
    "    \n",
    "    st_path = os.path.join(data_dir, 'steinmetz_st.npz')\n",
    "    lfp_path = os.path.join(data_dir, 'steinmetz_lfp.npz')\n",
    "    wav_path = os.path.join(data_dir, 'steinmetz_wav.npz')\n",
    "    \n",
    "    st_data = np.load(st_path, allow_pickle=True)['dat']\n",
    "    lfp_data = np.load(lfp_path, allow_pickle=True)['dat']\n",
    "    wav_data = np.load(wav_path, allow_pickle=True)['dat']\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(st_data)} sessions from spike times data\")\n",
    "        print(f\"Loaded {len(lfp_data)} sessions from LFP data\")\n",
    "        print(f\"Loaded {len(wav_data)} sessions from waveform data\")\n",
    "        \n",
    "        # Print keys for the first session of each dataset\n",
    "        print(\"\\nKeys in first session of each dataset:\")\n",
    "        print(f\"Spike times data: {list(st_data[0].keys())}\")\n",
    "        print(f\"LFP data: {list(lfp_data[0].keys())}\")\n",
    "        print(f\"Waveform data: {list(wav_data[0].keys())}\")\n",
    "    \n",
    "    return st_data, lfp_data, wav_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: Data Integration Function\n",
    "def integrate_steinmetz_data(st_data, lfp_data, wav_data, verbose=True):\n",
    "    \"\"\"\n",
    "    Integrate data from all three Steinmetz datasets\n",
    "    \n",
    "    Parameters:\n",
    "    st_data : numpy.ndarray\n",
    "        Spike times data\n",
    "    lfp_data : numpy.ndarray\n",
    "        LFP data\n",
    "    wav_data : numpy.ndarray\n",
    "        Waveform data\n",
    "    verbose : bool\n",
    "        Whether to print detailed information\n",
    "        \n",
    "    Returns:\n",
    "    integrated_data : list\n",
    "        List of dictionaries, each containing integrated data for one session\n",
    "    \"\"\"\n",
    "    # Verify all datasets have the same number of sessions\n",
    "    num_st_sessions = len(st_data)\n",
    "    num_lfp_sessions = len(lfp_data)\n",
    "    num_wav_sessions = len(wav_data)\n",
    "    \n",
    "    if not (num_st_sessions == num_lfp_sessions == num_wav_sessions):\n",
    "        print(\"WARNING: The datasets have different numbers of sessions!\")\n",
    "        num_sessions = min(num_st_sessions, num_lfp_sessions, num_wav_sessions)\n",
    "        print(f\"Will proceed with the first {num_sessions} sessions only\")\n",
    "    else:\n",
    "        num_sessions = num_st_sessions\n",
    "        if verbose:\n",
    "            print(f\"All datasets have {num_sessions} sessions\")\n",
    "    \n",
    "    # Create integrated dataset\n",
    "    integrated_data = []\n",
    "    \n",
    "    for i in range(num_sessions):\n",
    "        if verbose and i % 10 == 0:\n",
    "            print(f\"Integrating session {i}...\")\n",
    "        \n",
    "        # Create a new dictionary for this session\n",
    "        session_data = {}\n",
    "        \n",
    "        # Add all keys from spike times data\n",
    "        for key in st_data[i].keys():\n",
    "            session_data[key] = st_data[i][key]\n",
    "        \n",
    "        # Add all keys from LFP data\n",
    "        for key in lfp_data[i].keys():\n",
    "            # Avoid key collisions by adding prefix if key already exists\n",
    "            if key in session_data:\n",
    "                session_data[f\"lfp_{key}\"] = lfp_data[i][key]\n",
    "            else:\n",
    "                session_data[key] = lfp_data[i][key]\n",
    "        \n",
    "        # Add all keys from waveform data\n",
    "        for key in wav_data[i].keys():\n",
    "            # Avoid key collisions by adding prefix if key already exists\n",
    "            if key in session_data:\n",
    "                session_data[f\"wav_{key}\"] = wav_data[i][key]\n",
    "            else:\n",
    "                session_data[key] = wav_data[i][key]\n",
    "        \n",
    "        # Add the session to the list\n",
    "        integrated_data.append(session_data)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Successfully integrated {len(integrated_data)} sessions\")\n",
    "        print(f\"Keys in first integrated session: {list(integrated_data[0].keys())}\")\n",
    "    \n",
    "    return integrated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 4: Verification Function\n",
    "def verify_data_correspondence(integrated_data, session_idx=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Verify that data from different datasets corresponds correctly\n",
    "    \n",
    "    Parameters:\n",
    "    integrated_data : list\n",
    "        List of integrated session data\n",
    "    session_idx : int\n",
    "        Index of session to verify\n",
    "    verbose : bool\n",
    "        Whether to print detailed information\n",
    "        \n",
    "    Returns:\n",
    "    correspondence : dict\n",
    "        Dictionary with verification results\n",
    "    \"\"\"\n",
    "    if session_idx >= len(integrated_data):\n",
    "        print(f\"Session index {session_idx} out of range (max: {len(integrated_data)-1})\")\n",
    "        return None\n",
    "    \n",
    "    session = integrated_data[session_idx]\n",
    "    correspondence = {\"session_idx\": session_idx, \"checks\": {}}\n",
    "    \n",
    "    # Check 1: Neuron count consistency\n",
    "    if 'ss' in session and 'waveform_w' in session:\n",
    "        num_neurons_st = len(session['ss'])\n",
    "        num_neurons_wav = session['waveform_w'].shape[0]\n",
    "        \n",
    "        correspondence[\"checks\"][\"neuron_count_match\"] = (num_neurons_st == num_neurons_wav)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Neuron count in spike times data: {num_neurons_st}\")\n",
    "            print(f\"Neuron count in waveform data: {num_neurons_wav}\")\n",
    "            print(f\"Neuron counts match: {correspondence['checks']['neuron_count_match']}\")\n",
    "    \n",
    "    # Check 2: Trial count consistency\n",
    "    if 'ss' in session and len(session['ss']) > 0:\n",
    "        num_trials = len(session['ss'][0])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Number of trials: {num_trials}\")\n",
    "        \n",
    "        correspondence[\"checks\"][\"trial_count\"] = num_trials\n",
    "    \n",
    "    # Check 3: LFP data shape\n",
    "    if 'lfp' in session:\n",
    "        lfp_shape = session['lfp'].shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"LFP data shape: {lfp_shape}\")\n",
    "        \n",
    "        correspondence[\"checks\"][\"lfp_shape\"] = lfp_shape\n",
    "    \n",
    "    # Check 4: Brain areas\n",
    "    if 'brain_area' in session:\n",
    "        unique_areas = np.unique(session['brain_area'])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Number of unique brain areas: {len(unique_areas)}\")\n",
    "            print(f\"Brain areas: {unique_areas}\")\n",
    "        \n",
    "        correspondence[\"checks\"][\"brain_areas\"] = unique_areas.tolist()\n",
    "    \n",
    "    # Check 5: LFP brain areas\n",
    "    if 'brain_area_lfp' in session:\n",
    "        lfp_areas = session['brain_area_lfp']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Number of LFP brain areas: {len(lfp_areas)}\")\n",
    "            print(f\"LFP brain areas: {lfp_areas}\")\n",
    "        \n",
    "        correspondence[\"checks\"][\"lfp_brain_areas\"] = lfp_areas.tolist()\n",
    "    \n",
    "    return correspondence\n",
    "\n",
    "# Cell 5: Brain Region Grouping\n",
    "def define_brain_region_groups():\n",
    "    \"\"\"\n",
    "    Define groupings of brain regions\n",
    "    \n",
    "    Returns:\n",
    "    regions : list\n",
    "        List of region group names\n",
    "    brain_groups : list\n",
    "        List of lists, each containing brain areas in a group\n",
    "    \"\"\"\n",
    "    regions = [\"vis ctx\", \"thal\", \"hipp\", \"other ctx\", \"midbrain\", \"basal ganglia\", \"cortical subplate\", \"other\"]\n",
    "    \n",
    "    brain_groups = [\n",
    "        [\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\"],  # visual cortex\n",
    "        [\"CL\", \"LD\", \"LGd\", \"LH\", \"LP\", \"MD\", \"MG\", \"PO\", \"POL\", \"PT\", \"RT\", \"SPF\", \"TH\", \"VAL\", \"VPL\", \"VPM\"],  # thalamus\n",
    "        [\"CA\", \"CA1\", \"CA2\", \"CA3\", \"DG\", \"SUB\", \"POST\"],  # hippocampal\n",
    "        [\"ACA\", \"AUD\", \"COA\", \"DP\", \"ILA\", \"MOp\", \"MOs\", \"OLF\", \"ORB\", \"ORBm\", \"PIR\", \"PL\", \"SSp\", \"SSs\", \"RSP\", \"TT\"],  # non-visual cortex\n",
    "        [\"APN\", \"IC\", \"MB\", \"MRN\", \"NB\", \"PAG\", \"RN\", \"SCs\", \"SCm\", \"SCig\", \"SCsg\", \"ZI\"],  # midbrain\n",
    "        [\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\"],  # basal ganglia\n",
    "        [\"BLA\", \"BMA\", \"EP\", \"EPd\", \"MEA\"]  # cortical subplate\n",
    "    ]\n",
    "    \n",
    "    return regions, brain_groups\n",
    "\n",
    "# Cell 6: Session Selection Function\n",
    "def find_sessions_with_areas(integrated_data, areas_of_interest, min_neuron_count=5):\n",
    "    \"\"\"\n",
    "    Find sessions containing neurons from specific brain areas\n",
    "    \n",
    "    Parameters:\n",
    "    integrated_data : list\n",
    "        List of integrated session data\n",
    "    areas_of_interest : list\n",
    "        List of brain areas to look for\n",
    "    min_neuron_count : int\n",
    "        Minimum number of neurons required in each area\n",
    "        \n",
    "    Returns:\n",
    "    matching_sessions : list\n",
    "        List of dictionaries with session indices and neuron counts\n",
    "    \"\"\"\n",
    "    matching_sessions = []\n",
    "    \n",
    "    for i, session in enumerate(integrated_data):\n",
    "        if 'brain_area' not in session:\n",
    "            continue\n",
    "        \n",
    "        # Count neurons in each area of interest\n",
    "        area_counts = {}\n",
    "        for area in areas_of_interest:\n",
    "            area_counts[area] = np.sum(np.array(session['brain_area']) == area)\n",
    "        \n",
    "        # Check if all areas have enough neurons\n",
    "        if all(area_counts[area] >= min_neuron_count for area in areas_of_interest):\n",
    "            matching_sessions.append({\n",
    "                \"session_idx\": i,\n",
    "                \"neuron_counts\": area_counts\n",
    "            })\n",
    "    \n",
    "    return matching_sessions\n",
    "\n",
    "# Cell 7: Data Extraction Function\n",
    "def extract_session_data(integrated_data, session_idx, areas_of_interest=None):\n",
    "    \"\"\"\n",
    "    Extract and preprocess data for a specific session\n",
    "    \n",
    "    Parameters:\n",
    "    integrated_data : list\n",
    "        List of integrated session data\n",
    "    session_idx : int\n",
    "        Index of session to extract\n",
    "    areas_of_interest : list, optional\n",
    "        List of brain areas to focus on\n",
    "        \n",
    "    Returns:\n",
    "    extracted_data : dict\n",
    "        Dictionary with extracted and preprocessed data\n",
    "    \"\"\"\n",
    "    if session_idx >= len(integrated_data):\n",
    "        print(f\"Session index {session_idx} out of range (max: {len(integrated_data)-1})\")\n",
    "        return None\n",
    "    \n",
    "    session = integrated_data[session_idx]\n",
    "    extracted_data = {\"session_idx\": session_idx}\n",
    "    \n",
    "    # Extract spike data\n",
    "    if 'ss' in session:\n",
    "        extracted_data[\"spike_times\"] = session['ss']\n",
    "        \n",
    "        # Filter by brain area if specified\n",
    "        if areas_of_interest is not None and 'brain_area' in session:\n",
    "            area_mask = np.array([area in areas_of_interest for area in session['brain_area']])\n",
    "            extracted_data[\"area_mask\"] = area_mask\n",
    "            extracted_data[\"filtered_spike_times\"] = [session['ss'][i] for i in np.where(area_mask)[0]]\n",
    "            extracted_data[\"filtered_brain_areas\"] = np.array(session['brain_area'])[area_mask]\n",
    "    \n",
    "    # Extract LFP data\n",
    "    if 'lfp' in session:\n",
    "        extracted_data[\"lfp\"] = session['lfp']\n",
    "        \n",
    "        # Filter by brain area if specified\n",
    "        if areas_of_interest is not None and 'brain_area_lfp' in session:\n",
    "            lfp_area_mask = np.array([area in areas_of_interest for area in session['brain_area_lfp']])\n",
    "            extracted_data[\"lfp_area_mask\"] = lfp_area_mask\n",
    "            \n",
    "            if np.any(lfp_area_mask):\n",
    "                extracted_data[\"filtered_lfp\"] = session['lfp'][lfp_area_mask]\n",
    "                extracted_data[\"filtered_lfp_areas\"] = np.array(session['brain_area_lfp'])[lfp_area_mask]\n",
    "    \n",
    "    # Extract waveform data\n",
    "    if 'waveform_w' in session:\n",
    "        extracted_data[\"waveform_w\"] = session['waveform_w']\n",
    "        \n",
    "        # Filter by brain area if specified\n",
    "        if areas_of_interest is not None and 'brain_area' in session and 'area_mask' in extracted_data:\n",
    "            extracted_data[\"filtered_waveform_w\"] = session['waveform_w'][np.where(extracted_data[\"area_mask\"])[0]]\n",
    "    \n",
    "    if 'waveform_u' in session:\n",
    "        extracted_data[\"waveform_u\"] = session['waveform_u']\n",
    "        \n",
    "        # Filter by brain area if specified\n",
    "        if areas_of_interest is not None and 'brain_area' in session and 'area_mask' in extracted_data:\n",
    "            extracted_data[\"filtered_waveform_u\"] = session['waveform_u'][np.where(extracted_data[\"area_mask\"])[0]]\n",
    "    \n",
    "    # Extract trial information if available\n",
    "    for key in ['contrast_left', 'contrast_right', 'response', 'reaction_time', 'feedback_type']:\n",
    "        if key in session:\n",
    "            extracted_data[key] = session[key]\n",
    "    \n",
    "    return extracted_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and integrating Steinmetz datasets...\n",
      "Loading datasets...\n",
      "Loaded 39 sessions from spike times data\n",
      "Loaded 39 sessions from LFP data\n",
      "Loaded 39 sessions from waveform data\n",
      "\n",
      "Keys in first session of each dataset:\n",
      "Spike times data: ['ss', 'ss_passive']\n",
      "LFP data: ['lfp', 'lfp_passive', 'brain_area_lfp']\n",
      "Waveform data: ['waveform_w', 'waveform_u', 'trough_to_peak']\n",
      "All datasets have 39 sessions\n",
      "Integrating session 0...\n",
      "Integrating session 10...\n",
      "Integrating session 20...\n",
      "Integrating session 30...\n",
      "Successfully integrated 39 sessions\n",
      "Keys in first integrated session: ['ss', 'ss_passive', 'lfp', 'lfp_passive', 'brain_area_lfp', 'waveform_w', 'waveform_u', 'trough_to_peak']\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Example Usage - Data Loading and Integration\n",
    "# Run this cell to load and integrate the data\n",
    "print(\"Loading and integrating Steinmetz datasets...\")\n",
    "st_data, lfp_data, wav_data = load_steinmetz_data()\n",
    "integrated_data = integrate_steinmetz_data(st_data, lfp_data, wav_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying data correspondence for session 0...\n",
      "Neuron count in spike times data: 734\n",
      "Neuron count in waveform data: 734\n",
      "Neuron counts match: True\n",
      "Number of trials: 214\n",
      "LFP data shape: (7, 214, 250)\n",
      "Number of LFP brain areas: 7\n",
      "LFP brain areas: [np.str_('ACA'), np.str_('LS'), np.str_('MOs'), np.str_('CA3'), np.str_('DG'), np.str_('SUB'), np.str_('VISp')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 9: Example Usage - Verify Data Correspondence\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Verify data correspondence for the first session\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mVerifying data correspondence for session 0...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m correspondence = \u001b[43mverify_data_correspondence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegrated_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mverify_data_correspondence\u001b[39m\u001b[34m(integrated_data, session_idx, verbose)\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of LFP brain areas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lfp_areas)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLFP brain areas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlfp_areas\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     correspondence[\u001b[33m\"\u001b[39m\u001b[33mchecks\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlfp_brain_areas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mlfp_areas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m()\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m correspondence\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 9: Example Usage - Verify Data Correspondence\n",
    "# Verify data correspondence for the first session\n",
    "print(\"\\nVerifying data correspondence for session 0...\")\n",
    "correspondence = verify_data_correspondence(integrated_data, session_idx=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Example Usage - Find Sessions with Areas of Interest\n",
    "# Define areas of interest for research questions\n",
    "areas_of_interest = ['MOs', 'ACB', 'CP', 'ACA', 'PL']  # MOs, basal ganglia, prefrontal cortex\n",
    "print(f\"\\nFinding sessions with neurons in {areas_of_interest}...\")\n",
    "matching_sessions = find_sessions_with_areas(integrated_data, areas_of_interest)\n",
    "\n",
    "print(f\"Found {len(matching_sessions)} sessions with neurons in all areas of interest\")\n",
    "if matching_sessions:\n",
    "    print(\"Top 3 matching sessions:\")\n",
    "    for i, session in enumerate(matching_sessions[:3]):\n",
    "        print(f\"Session {session['session_idx']}: {session['neuron_counts']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Example Usage - Extract Data from a Session\n",
    "# Extract data from the first matching session if available\n",
    "if matching_sessions:\n",
    "    best_session_idx = matching_sessions[0][\"session_idx\"]\n",
    "    print(f\"\\nExtracting data from session {best_session_idx}...\")\n",
    "    session_data = extract_session_data(integrated_data, best_session_idx, areas_of_interest)\n",
    "    \n",
    "    # Print summary of extracted data\n",
    "    print(\"Extracted data summary:\")\n",
    "    for key, value in session_data.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"{key}: shape {value.shape}\")\n",
    "        elif isinstance(value, list) and key == \"filtered_spike_times\":\n",
    "            print(f\"{key}: {len(value)} neurons\")\n",
    "        elif not isinstance(value, list) and not isinstance(value, np.ndarray):\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Spike Train to Firing Rate Function\n",
    "def spike_times_to_firing_rate(spike_times, bin_size=0.01, t_start=-0.5, t_end=1.0):\n",
    "    \"\"\"\n",
    "    Convert spike times to binned firing rates\n",
    "    \n",
    "    Parameters:\n",
    "    spike_times : list\n",
    "        List of spike times for each neuron and trial\n",
    "    bin_size : float\n",
    "        Size of time bins in seconds\n",
    "    t_start : float\n",
    "        Start time in seconds (relative to stimulus onset)\n",
    "    t_end : float\n",
    "        End time in seconds (relative to stimulus onset)\n",
    "        \n",
    "    Returns:\n",
    "    firing_rates : numpy.ndarray\n",
    "        Array of firing rates (neurons x trials x time bins)\n",
    "    time_bins : numpy.ndarray\n",
    "        Array of time bin centers\n",
    "    \"\"\"\n",
    "    if not spike_times:\n",
    "        return None, None\n",
    "    \n",
    "    # Create time bins\n",
    "    time_bins = np.arange(t_start, t_end, bin_size)\n",
    "    bin_centers = time_bins[:-1] + bin_size/2\n",
    "    \n",
    "    n_neurons = len(spike_times)\n",
    "    n_trials = len(spike_times[0])\n",
    "    n_bins = len(time_bins) - 1\n",
    "    \n",
    "    # Initialize firing rates array\n",
    "    firing_rates = np.zeros((n_neurons, n_trials, n_bins))\n",
    "    \n",
    "    # Bin spike times into firing rates\n",
    "    for n in range(n_neurons):\n",
    "        for t in range(n_trials):\n",
    "            spikes = np.array(spike_times[n][t])\n",
    "            for b in range(n_bins):\n",
    "                bin_start = time_bins[b]\n",
    "                bin_end = time_bins[b+1]\n",
    "                firing_rates[n, t, b] = np.sum((spikes >= bin_start) & (spikes < bin_end)) / bin_size\n",
    "    \n",
    "    return firing_rates, bin_centers\n",
    "\n",
    "# Cell 13: LFP Analysis Function\n",
    "def analyze_lfp(lfp_data, brain_areas, sampling_rate=100):\n",
    "    \"\"\"\n",
    "    Analyze LFP data: compute power spectra and filter in frequency bands\n",
    "    \n",
    "    Parameters:\n",
    "    lfp_data : numpy.ndarray\n",
    "        LFP data (channels x time)\n",
    "    brain_areas : list\n",
    "        List of brain areas for each channel\n",
    "    sampling_rate : float\n",
    "        Sampling rate in Hz\n",
    "        \n",
    "    Returns:\n",
    "    lfp_analysis : dict\n",
    "        Dictionary with LFP analysis results\n",
    "    \"\"\"\n",
    "    n_channels, n_timepoints = lfp_data.shape\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    lfp_analysis = {\n",
    "        \"brain_areas\": brain_areas,\n",
    "        \"power_spectra\": [],\n",
    "        \"frequencies\": None,\n",
    "        \"theta_power\": np.zeros(n_channels),\n",
    "        \"beta_power\": np.zeros(n_channels),\n",
    "        \"gamma_power\": np.zeros(n_channels)\n",
    "    }\n",
    "    \n",
    "    # Compute power spectra for each channel\n",
    "    for ch in range(n_channels):\n",
    "        # Compute power spectrum\n",
    "        freqs, psd = signal.welch(lfp_data[ch], fs=sampling_rate, nperseg=256)\n",
    "        \n",
    "        # Store results\n",
    "        lfp_analysis[\"power_spectra\"].append(psd)\n",
    "        if lfp_analysis[\"frequencies\"] is None:\n",
    "            lfp_analysis[\"frequencies\"] = freqs\n",
    "        \n",
    "        # Compute power in specific frequency bands\n",
    "        theta_mask = (freqs >= 4) & (freqs <= 8)\n",
    "        beta_mask = (freqs >= 13) & (freqs <= 30)\n",
    "        gamma_mask = (freqs >= 30) & (freqs <= 80)\n",
    "        \n",
    "        lfp_analysis[\"theta_power\"][ch] = np.mean(psd[theta_mask])\n",
    "        lfp_analysis[\"beta_power\"][ch] = np.mean(psd[beta_mask])\n",
    "        lfp_analysis[\"gamma_power\"][ch] = np.mean(psd[gamma_mask])\n",
    "    \n",
    "    # Convert power spectra to array\n",
    "    lfp_analysis[\"power_spectra\"] = np.array(lfp_analysis[\"power_spectra\"])\n",
    "    \n",
    "    return lfp_analysis\n",
    "\n",
    "# Cell 14: Cross-Regional Connectivity Function\n",
    "def compute_cross_regional_connectivity(firing_rates, brain_areas):\n",
    "    \"\"\"\n",
    "    Compute functional connectivity between brain regions\n",
    "    \n",
    "    Parameters:\n",
    "    firing_rates : numpy.ndarray\n",
    "        Firing rates (neurons x trials x time)\n",
    "    brain_areas : numpy.ndarray\n",
    "        Brain area for each neuron\n",
    "        \n",
    "    Returns:\n",
    "    connectivity : dict\n",
    "        Dictionary with connectivity results\n",
    "    \"\"\"\n",
    "    # Get unique brain areas\n",
    "    unique_areas = np.unique(brain_areas)\n",
    "    n_areas = len(unique_areas)\n",
    "    \n",
    "    # Average firing rates across trials\n",
    "    mean_rates = np.mean(firing_rates, axis=1)  # neurons x time\n",
    "    \n",
    "    # Compute correlation matrix between all neurons\n",
    "    corr_matrix = np.corrcoef(mean_rates)\n",
    "    \n",
    "    # Compute average correlation between brain areas\n",
    "    area_corr = np.zeros((n_areas, n_areas))\n",
    "    \n",
    "    for i, area1 in enumerate(unique_areas):\n",
    "        for j, area2 in enumerate(unique_areas):\n",
    "            # Get indices of neurons in each area\n",
    "            idx1 = np.where(brain_areas == area1)[0]\n",
    "            idx2 = np.where(brain_areas == area2)[0]\n",
    "            \n",
    "            # Compute average correlation between areas\n",
    "            if i == j:  # Within-area correlation\n",
    "                # Exclude self-correlations (diagonal)\n",
    "                area_corr[i, j] = np.mean(corr_matrix[np.ix_(idx1, idx1)] - np.eye(len(idx1)))\n",
    "            else:  # Between-area correlation\n",
    "                area_corr[i, j] = np.mean(corr_matrix[np.ix_(idx1, idx2)])\n",
    "    \n",
    "    # Create connectivity dictionary\n",
    "    connectivity = {\n",
    "        \"neuron_corr\": corr_matrix,\n",
    "        \"area_corr\": area_corr,\n",
    "        \"areas\": unique_areas\n",
    "    }\n",
    "    \n",
    "    return connectivity\n",
    "\n",
    "# Cell 15: Neural Dynamics Analysis Function\n",
    "def analyze_neural_dynamics(firing_rates, brain_areas, trial_info=None):\n",
    "    \"\"\"\n",
    "    Analyze neural dynamics using dimensionality reduction\n",
    "    \n",
    "    Parameters:\n",
    "    firing_rates : numpy.ndarray\n",
    "        Firing rates (neurons x trials x time)\n",
    "    brain_areas : numpy.ndarray\n",
    "        Brain area for each neuron\n",
    "    trial_info : dict, optional\n",
    "        Dictionary with trial information\n",
    "        \n",
    "    Returns:\n",
    "    dynamics : dict\n",
    "        Dictionary with neural dynamics results\n",
    "    \"\"\"\n",
    "    # Get unique brain areas\n",
    "    unique_areas = np.unique(brain_areas)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    dynamics = {\n",
    "        \"areas\": unique_areas,\n",
    "        \"pca_results\": {},\n",
    "        \"trajectories\": {}\n",
    "    }\n",
    "    \n",
    "    # Analyze each brain area separately\n",
    "    for area in unique_areas:\n",
    "        # Get neurons in this area\n",
    "        area_mask = brain_areas == area\n",
    "        area_rates = firing_rates[area_mask]\n",
    "        \n",
    "        if len(area_rates) < 3:\n",
    "            # Skip areas with too few neurons\n",
    "            continue\n",
    "        \n",
    "        # Reshape for PCA: (neurons x (trials*time))\n",
    "        n_neurons, n_trials, n_timepoints = area_rates.shape\n",
    "        reshaped_rates = area_rates.reshape(n_neurons, -1).T\n",
    "        \n",
    "        # Run PCA\n",
    "        pca = PCA(n_components=min(3, n_neurons))\n",
    "        pca_result = pca.fit_transform(reshaped_rates)\n",
    "        \n",
    "        # Reshape back to (trials x time x components)\n",
    "        trajectories = pca_result.T.reshape(min(3, n_neurons), n_trials, n_timepoints)\n",
    "        \n",
    "        # Store results\n",
    "        dynamics[\"pca_results\"][area] = {\n",
    "            \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "            \"components\": pca.components_\n",
    "        }\n",
    "        dynamics[\"trajectories\"][area] = trajectories\n",
    "    \n",
    "    # If trial information is provided, add condition-specific trajectories\n",
    "    if trial_info is not None and 'response' in trial_info:\n",
    "        dynamics[\"condition_trajectories\"] = {}\n",
    "        \n",
    "        for area in unique_areas:\n",
    "            if area not in dynamics[\"trajectories\"]:\n",
    "                continue\n",
    "            \n",
    "            # Get trajectories for this area\n",
    "            area_trajectories = dynamics[\"trajectories\"][area]\n",
    "            \n",
    "            # Split by response\n",
    "            left_mask = trial_info['response'] == -1\n",
    "            right_mask = trial_info['response'] == 1\n",
    "            \n",
    "            dynamics[\"condition_trajectories\"][area] = {\n",
    "                \"left\": area_trajectories[:, left_mask, :],\n",
    "                \"right\": area_trajectories[:, right_mask, :]\n",
    "            }\n",
    "    \n",
    "    return dynamics\n",
    "\n",
    "# Cell 16: Example Usage - Compute Firing Rates\n",
    "# Compute firing rates for the extracted session data\n",
    "if 'session_data' in locals() and 'filtered_spike_times' in session_data:\n",
    "    print(\"\\nComputing firing rates...\")\n",
    "    firing_rates, time_bins = spike_times_to_firing_rate(session_data[\"filtered_spike_times\"])\n",
    "    \n",
    "    if firing_rates is not None:\n",
    "        print(f\"Firing rates shape: {firing_rates.shape}\")\n",
    "        print(f\"Time bins: {time_bins[0]} to {time_bins[-1]} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 17: Example Usage - Analyze LFP\n",
    "# Analyze LFP data for the extracted session\n",
    "if 'session_data' in locals() and 'filtered_lfp' in session_data and 'filtered_lfp_areas' in session_data:\n",
    "    print(\"\\nAnalyzing LFP data...\")\n",
    "    lfp_analysis = analyze_lfp(session_data[\"filtered_lfp\"], session_data[\"filtered_lfp_areas\"])\n",
    "    \n",
    "    print(\"LFP analysis results:\")\n",
    "    print(f\"Analyzed {len(lfp_analysis['brain_areas'])} LFP channels\")\n",
    "    print(f\"Frequency range: {lfp_analysis['frequencies'][0]} to {lfp_analysis['frequencies'][-1]} Hz\")\n",
    "    \n",
    "    # Print average power in different frequency bands for each area\n",
    "    unique_lfp_areas = np.unique(session_data[\"filtered_lfp_areas\"])\n",
    "    print(\"\\nAverage power by brain area and frequency band:\")\n",
    "    for area in unique_lfp_areas:\n",
    "        area_mask = np.array(session_data[\"filtered_lfp_areas\"]) == area\n",
    "        print(f\"{area}:\")\n",
    "        print(f\"  Theta (4-8 Hz): {np.mean(lfp_analysis['theta_power'][area_mask]):.2f}\")\n",
    "        print(f\"  Beta (13-30 Hz): {np.mean(lfp_analysis['beta_power'][area_mask]):.2f}\")\n",
    "        print(f\"  Gamma (30-80 Hz): {np.mean(lfp_analysis['gamma_power'][area_mask]):.2f}\")\n",
    "\n",
    "# Cell 18: Example Usage - Compute Cross-Regional Connectivity\n",
    "# Compute functional connectivity between brain regions\n",
    "if 'firing_rates' in locals() and 'session_data' in locals() and 'filtered_brain_areas' in session_data:\n",
    "    print(\"\\nComputing cross-regional connectivity...\")\n",
    "    connectivity = compute_cross_regional_connectivity(firing_rates, session_data[\"filtered_brain_areas\"])\n",
    "    \n",
    "    print(\"Connectivity results:\")\n",
    "    print(f\"Analyzed {len(connectivity['areas'])} brain areas: {connectivity['areas']}\")\n",
    "    \n",
    "    # Plot connectivity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(connectivity['area_corr'], annot=True, cmap='coolwarm', \n",
    "                xticklabels=connectivity['areas'], yticklabels=connectivity['areas'])\n",
    "    plt.title('Cross-Regional Functional Connectivity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cell 19: Example Usage - Analyze Neural Dynamics\n",
    "# Analyze neural dynamics using dimensionality reduction\n",
    "if 'firing_rates' in locals() and 'session_data' in locals() and 'filtered_brain_areas' in session_data:\n",
    "    print(\"\\nAnalyzing neural dynamics...\")\n",
    "    \n",
    "    # Create trial info dictionary\n",
    "    trial_info = {}\n",
    "    for key in ['response', 'contrast_left', 'contrast_right']:\n",
    "        if key in session_data:\n",
    "            trial_info[key] = session_data[key]\n",
    "    \n",
    "    dynamics = analyze_neural_dynamics(firing_rates, session_data[\"filtered_brain_areas\"], trial_info)\n",
    "    \n",
    "    print(\"Neural dynamics results:\")\n",
    "    print(f\"Analyzed {len(dynamics['areas'])} brain areas: {dynamics['areas']}\")\n",
    "    \n",
    "    # Plot explained variance for each area\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, area in enumerate(dynamics['pca_results'].keys()):\n",
    "        plt.subplot(1, len(dynamics['pca_results']), i+1)\n",
    "        plt.bar(range(1, len(dynamics['pca_results'][area]['explained_variance_ratio'])+1), \n",
    "                dynamics['pca_results'][area]['explained_variance_ratio'])\n",
    "        plt.title(f'{area} Explained Variance')\n",
    "        plt.xlabel('PC')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cell 20: Visualization Function - Plot Neural Trajectories\n",
    "def plot_neural_trajectories(dynamics, area, condition='all'):\n",
    "    \"\"\"\n",
    "    Plot neural trajectories in PCA space\n",
    "    \n",
    "    Parameters:\n",
    "    dynamics : dict\n",
    "        Dictionary with neural dynamics results from analyze_neural_dynamics\n",
    "    area : str\n",
    "        Brain area to plot\n",
    "    condition : str\n",
    "        'all', 'left', or 'right' to plot all trials or specific conditions\n",
    "    \"\"\"\n",
    "    if area not in dynamics['trajectories']:\n",
    "        print(f\"Area {area} not found in dynamics results\")\n",
    "        return\n",
    "    \n",
    "    # Get trajectories for this area\n",
    "    if condition == 'all':\n",
    "        trajectories = dynamics['trajectories'][area]\n",
    "        n_components, n_trials, n_timepoints = trajectories.shape\n",
    "        \n",
    "        # Plot average trajectory\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 3D plot if we have 3 components\n",
    "        if n_components >= 3:\n",
    "            ax = plt.subplot(111, projection='3d')\n",
    "            mean_traj = np.mean(trajectories, axis=1)\n",
    "            ax.plot(mean_traj[0], mean_traj[1], mean_traj[2], 'k-', linewidth=2)\n",
    "            \n",
    "            # Add points at specific timepoints\n",
    "            timepoints = [0, n_timepoints//4, n_timepoints//2, 3*n_timepoints//4, n_timepoints-1]\n",
    "            colors = ['blue', 'cyan', 'green', 'orange', 'red']\n",
    "            \n",
    "            for i, t in enumerate(timepoints):\n",
    "                ax.scatter(mean_traj[0, t], mean_traj[1, t], mean_traj[2, t], \n",
    "                           color=colors[i], s=100, label=f'Time {t}')\n",
    "            \n",
    "            ax.set_xlabel('PC1')\n",
    "            ax.set_ylabel('PC2')\n",
    "            ax.set_zlabel('PC3')\n",
    "            ax.set_title(f'{area} Neural Trajectory')\n",
    "            ax.legend()\n",
    "        \n",
    "        # 2D plot if we have 2 components\n",
    "        elif n_components >= 2:\n",
    "            mean_traj = np.mean(trajectories, axis=1)\n",
    "            plt.plot(mean_traj[0], mean_traj[1], 'k-', linewidth=2)\n",
    "            \n",
    "            # Add points at specific timepoints\n",
    "            timepoints = [0, n_timepoints//4, n_timepoints//2, 3*n_timepoints//4, n_timepoints-1]\n",
    "            colors = ['blue', 'cyan', 'green', 'orange', 'red']\n",
    "            \n",
    "            for i, t in enumerate(timepoints):\n",
    "                plt.scatter(mean_traj[0, t], mean_traj[1, t], \n",
    "                           color=colors[i], s=100, label=f'Time {t}')\n",
    "            \n",
    "            plt.xlabel('PC1')\n",
    "            plt.ylabel('PC2')\n",
    "            plt.title(f'{area} Neural Trajectory')\n",
    "            plt.legend()\n",
    "    \n",
    "    # Plot condition-specific trajectories\n",
    "    elif condition in ['left', 'right'] and 'condition_trajectories' in dynamics:\n",
    "        if area not in dynamics['condition_trajectories']:\n",
    "            print(f\"Area {area} not found in condition trajectories\")\n",
    "            return\n",
    "        \n",
    "        trajectories = dynamics['condition_trajectories'][area][condition]\n",
    "        n_components, n_trials, n_timepoints = trajectories.shape\n",
    "        \n",
    "        # Plot average trajectory\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # 3D plot if we have 3 components\n",
    "        if n_components >= 3:\n",
    "            ax = plt.subplot(111, projection='3d')\n",
    "            mean_traj = np.mean(trajectories, axis=1)\n",
    "            ax.plot(mean_traj[0], mean_traj[1], mean_traj[2], 'k-', linewidth=2)\n",
    "            \n",
    "            # Add points at specific timepoints\n",
    "            timepoints = [0, n_timepoints//4, n_timepoints//2, 3*n_timepoints//4, n_timepoints-1]\n",
    "            colors = ['blue', 'cyan', 'green', 'orange', 'red']\n",
    "            \n",
    "            for i, t in enumerate(timepoints):\n",
    "                ax.scatter(mean_traj[0, t], mean_traj[1, t], mean_traj[2, t], \n",
    "                           color=colors[i], s=100, label=f'Time {t}')\n",
    "            \n",
    "            ax.set_xlabel('PC1')\n",
    "            ax.set_ylabel('PC2')\n",
    "            ax.set_zlabel('PC3')\n",
    "            ax.set_title(f'{area} Neural Trajectory - {condition.capitalize()} Trials')\n",
    "            ax.legend()\n",
    "        \n",
    "        # 2D plot if we have 2 components\n",
    "        elif n_components >= 2:\n",
    "            mean_traj = np.mean(trajectories, axis=1)\n",
    "            plt.plot(mean_traj[0], mean_traj[1], 'k-', linewidth=2)\n",
    "            \n",
    "            # Add points at specific timepoints\n",
    "            timepoints = [0, n_timepoints//4, n_timepoints//2, 3*n_timepoints//4, n_timepoints-1]\n",
    "            colors = ['blue', 'cyan', 'green', 'orange', 'red']\n",
    "            \n",
    "            for i, t in enumerate(timepoints):\n",
    "                plt.scatter(mean_traj[0, t], mean_traj[1, t], \n",
    "                           color=colors[i], s=100, label=f'Time {t}')\n",
    "            \n",
    "            plt.xlabel('PC1')\n",
    "            plt.ylabel('PC2')\n",
    "            plt.title(f'{area} Neural Trajectory - {condition.capitalize()} Trials')\n",
    "            plt.legend()\n",
    "    \n",
    "    else:\n",
    "        print(f\"Condition {condition} not recognized or condition trajectories not available\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cell 21: Example Usage - Plot Neural Trajectories\n",
    "# Plot neural trajectories for a specific brain area\n",
    "if 'dynamics' in locals():\n",
    "    # Find an area with enough neurons for PCA\n",
    "    for area in dynamics['areas']:\n",
    "        if area in dynamics['trajectories']:\n",
    "            print(f\"\\nPlotting neural trajectories for {area}...\")\n",
    "            plot_neural_trajectories(dynamics, area)\n",
    "            \n",
    "            # Plot condition-specific trajectories if available\n",
    "            if 'condition_trajectories' in dynamics and area in dynamics['condition_trajectories']:\n",
    "                plot_neural_trajectories(dynamics, area, 'left')\n",
    "                plot_neural_trajectories(dynamics, area, 'right')\n",
    "            \n",
    "            break\n",
    "\n",
    "# Cell 22: Age-Related Analysis Function\n",
    "def analyze_age_differences(integrated_data, young_sessions, old_sessions, areas_of_interest):\n",
    "    \"\"\"\n",
    "    Analyze age-related differences in neural activity and connectivity\n",
    "    \n",
    "    Parameters:\n",
    "    integrated_data : list\n",
    "        List of integrated session data\n",
    "    young_sessions : list\n",
    "        List of session indices for young mice\n",
    "    old_sessions : list\n",
    "        List of session indices for old mice\n",
    "    areas_of_interest : list\n",
    "        List of brain areas to analyze\n",
    "        \n",
    "    Returns:\n",
    "    age_analysis : dict\n",
    "        Dictionary with age-related analysis results\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    age_analysis = {\n",
    "        \"young\": {\"connectivity\": [], \"firing_rates\": []},\n",
    "        \"old\": {\"connectivity\": [], \"firing_rates\": []}\n",
    "    }\n",
    "    \n",
    "    # Analyze young sessions\n",
    "    for session_idx in young_sessions:\n",
    "        # Extract session data\n",
    "        session_data = extract_session_data(integrated_data, session_idx, areas_of_interest)\n",
    "        \n",
    "        if 'filtered_spike_times' in session_data:\n",
    "            # Compute firing rates\n",
    "            firing_rates, _ = spike_times_to_firing_rate(session_data[\"filtered_spike_times\"])\n",
    "            \n",
    "            if firing_rates is not None:\n",
    "                # Compute mean firing rate for each area\n",
    "                for area in np.unique(session_data[\"filtered_brain_areas\"]):\n",
    "                    area_mask = session_data[\"filtered_brain_areas\"] == area\n",
    "                    area_rates = firing_rates[area_mask]\n",
    "                    mean_rate = np.mean(area_rates)\n",
    "                    \n",
    "                    age_analysis[\"young\"][\"firing_rates\"].append({\n",
    "                        \"session_idx\": session_idx,\n",
    "                        \"area\": area,\n",
    "                        \"mean_rate\": mean_rate\n",
    "                    })\n",
    "                \n",
    "                # Compute connectivity\n",
    "                connectivity = compute_cross_regional_connectivity(firing_rates, session_data[\"filtered_brain_areas\"])\n",
    "                age_analysis[\"young\"][\"connectivity\"].append({\n",
    "                    \"session_idx\": session_idx,\n",
    "                    \"area_corr\": connectivity[\"area_corr\"],\n",
    "                    \"areas\": connectivity[\"areas\"]\n",
    "                })\n",
    "    \n",
    "    # Analyze old sessions\n",
    "    for session_idx in old_sessions:\n",
    "        # Extract session data\n",
    "        session_data = extract_session_data(integrated_data, session_idx, areas_of_interest)\n",
    "        \n",
    "        if 'filtered_spike_times' in session_data:\n",
    "            # Compute firing rates\n",
    "            firing_rates, _ = spike_times_to_firing_rate(session_data[\"filtered_spike_times\"])\n",
    "            \n",
    "            if firing_rates is not None:\n",
    "                # Compute mean firing rate for each area\n",
    "                for area in np.unique(session_data[\"filtered_brain_areas\"]):\n",
    "                    area_mask = session_data[\"filtered_brain_areas\"] == area\n",
    "                    area_rates = firing_rates[area_mask]\n",
    "                    mean_rate = np.mean(area_rates)\n",
    "                    \n",
    "                    age_analysis[\"old\"][\"firing_rates\"].append({\n",
    "                        \"session_idx\": session_idx,\n",
    "                        \"area\": area,\n",
    "                        \"mean_rate\": mean_rate\n",
    "                    })\n",
    "                \n",
    "                # Compute connectivity\n",
    "                connectivity = compute_cross_regional_connectivity(firing_rates, session_data[\"filtered_brain_areas\"])\n",
    "                age_analysis[\"old\"][\"connectivity\"].append({\n",
    "                    \"session_idx\": session_idx,\n",
    "                    \"area_corr\": connectivity[\"area_corr\"],\n",
    "                    \"areas\": connectivity[\"areas\"]\n",
    "                })\n",
    "    \n",
    "    return age_analysis\n",
    "\n",
    "# Cell 23: Example Usage - Age-Related Analysis\n",
    "# For demonstration purposes, let's assume the first half of sessions are from young mice\n",
    "# and the second half are from old mice\n",
    "if 'integrated_data' in locals():\n",
    "    num_sessions = len(integrated_data)\n",
    "    young_sessions = list(range(num_sessions // 2))\n",
    "    old_sessions = list(range(num_sessions // 2, num_sessions))\n",
    "    \n",
    "    print(\"\\nPerforming age-related analysis...\")\n",
    "    print(f\"Young sessions: {young_sessions[:5]}...\")\n",
    "    print(f\"Old sessions: {old_sessions[:5]}...\")\n",
    "    \n",
    "    # Define areas of interest\n",
    "    areas_of_interest = ['MOs', 'ACB', 'CP', 'ACA', 'PL']  # MOs, basal ganglia, prefrontal cortex\n",
    "    \n",
    "    # Note: This analysis would be computationally intensive and time-consuming\n",
    "    # For demonstration, we'll just show the function definition\n",
    "    print(\"Note: Age-related analysis would analyze differences in neural activity and connectivity\")\n",
    "    print(\"between young and old mice across multiple sessions.\")\n",
    "    print(\"This would be computationally intensive and is not executed in this notebook.\")\n",
    "\n",
    "# Cell 24: Visualization Function - Plot LFP Power Spectra\n",
    "def plot_lfp_power_spectra(lfp_analysis, areas_to_plot=None):\n",
    "    \"\"\"\n",
    "    Plot LFP power spectra for specific brain areas\n",
    "    \n",
    "    Parameters:\n",
    "    lfp_analysis : dict\n",
    "        Dictionary with LFP analysis results from analyze_lfp\n",
    "    areas_to_plot : list, optional\n",
    "        List of brain areas to plot. If None, plot all areas.\n",
    "    \"\"\"\n",
    "    # Get unique brain areas\n",
    "    unique_areas = np.unique(lfp_analysis[\"brain_areas\"])\n",
    "    \n",
    "    # Filter areas if specified\n",
    "    if areas_to_plot is not None:\n",
    "        plot_areas = [area for area in areas_to_plot if area in unique_areas]\n",
    "    else:\n",
    "        plot_areas = unique_areas\n",
    "    \n",
    "    # Plot power spectra\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, area in enumerate(plot_areas):\n",
    "        # Get channels for this area\n",
    "        area_mask = np.array(lfp_analysis[\"brain_areas\"]) == area\n",
    "        area_spectra = lfp_analysis[\"power_spectra\"][area_mask]\n",
    "        \n",
    "        # Plot average spectrum\n",
    "        plt.subplot(len(plot_areas), 1, i+1)\n",
    "        mean_spectrum = np.mean(area_spectra, axis=0)\n",
    "        std_spectrum = np.std(area_spectra, axis=0)\n",
    "        \n",
    "        plt.plot(lfp_analysis[\"frequencies\"], mean_spectrum, 'k-', linewidth=2)\n",
    "        plt.fill_between(lfp_analysis[\"frequencies\"], \n",
    "                         mean_spectrum - std_spectrum, \n",
    "                         mean_spectrum + std_spectrum, \n",
    "                         alpha=0.3)\n",
    "        \n",
    "        # Mark frequency bands\n",
    "        plt.axvspan(4, 8, alpha=0.2, color='blue', label='Theta (4-8 Hz)')\n",
    "        plt.axvspan(13, 30, alpha=0.2, color='green', label='Beta (13-30 Hz)')\n",
    "        plt.axvspan(30, 80, alpha=0.2, color='red', label='Gamma (30-80 Hz)')\n",
    "        \n",
    "        plt.title(f'{area} LFP Power Spectrum')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Power')\n",
    "        plt.xlim(0, 100)  # Limit to 0-100 Hz for better visualization\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cell 25: Example Usage - Plot LFP Power Spectra\n",
    "# Plot LFP power spectra for specific brain areas\n",
    "if 'lfp_analysis' in locals():\n",
    "    print(\"\\nPlotting LFP power spectra...\")\n",
    "    \n",
    "    # Get unique brain areas\n",
    "    unique_areas = np.unique(lfp_analysis[\"brain_areas\"])\n",
    "    print(f\"Available areas: {unique_areas}\")\n",
    "    \n",
    "    # Plot power spectra for up to 3 areas\n",
    "    areas_to_plot = unique_areas[:min(3, len(unique_areas))]\n",
    "    plot_lfp_power_spectra(lfp_analysis, areas_to_plot)\n",
    "\n",
    "# Cell 26: Visualization Function - Plot Firing Rate Heatmap\n",
    "def plot_firing_rate_heatmap(firing_rates, brain_areas, time_bins):\n",
    "    \"\"\"\n",
    "    Plot firing rate heatmap for neurons grouped by brain area\n",
    "    \n",
    "    Parameters:\n",
    "    firing_rates : numpy.ndarray\n",
    "        Firing rates (neurons x trials x time)\n",
    "    brain_areas : numpy.ndarray\n",
    "        Brain area for each neuron\n",
    "    time_bins : numpy.ndarray\n",
    "        Time bin centers\n",
    "    \"\"\"\n",
    "    # Get unique brain areas\n",
    "    unique_areas = np.unique(brain_areas)\n",
    "    \n",
    "    # Compute mean firing rate across trials\n",
    "    mean_rates = np.mean(firing_rates, axis=1)  # neurons x time\n",
    "    \n",
    "    # Sort neurons by brain area\n",
    "    sorted_indices = np.argsort(brain_areas)\n",
    "    sorted_rates = mean_rates[sorted_indices]\n",
    "    sorted_areas = brain_areas[sorted_indices]\n",
    "    \n",
    "    # Create area boundaries for plotting\n",
    "    area_boundaries = []\n",
    "    current_area = sorted_areas[0]\n",
    "    current_count = 0\n",
    "    \n",
    "    for area in sorted_areas:\n",
    "        if area == current_area:\n",
    "            current_count += 1\n",
    "        else:\n",
    "            area_boundaries.append((current_area, current_count))\n",
    "            current_area = area\n",
    "            current_count = 1\n",
    "    \n",
    "    area_boundaries.append((current_area, current_count))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot firing rate heatmap\n",
    "    plt.imshow(sorted_rates, aspect='auto', cmap='viridis', \n",
    "               extent=[time_bins[0], time_bins[-1], len(sorted_rates), 0])\n",
    "    \n",
    "    # Add area labels\n",
    "    y_pos = 0\n",
    "    for area, count in area_boundaries:\n",
    "        plt.axhline(y_pos + count, color='white', linestyle='-')\n",
    "        plt.text(time_bins[-1] + 0.05, y_pos + count/2, area, \n",
    "                 verticalalignment='center', fontsize=10)\n",
    "        y_pos += count\n",
    "    \n",
    "    # Add stimulus onset line\n",
    "    plt.axvline(0, color='red', linestyle='--', label='Stimulus Onset')\n",
    "    \n",
    "    plt.colorbar(label='Firing Rate (Hz)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Neuron')\n",
    "    plt.title('Firing Rate Heatmap by Brain Area')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cell 27: Example Usage - Plot Firing Rate Heatmap\n",
    "# Plot firing rate heatmap for neurons grouped by brain area\n",
    "if 'firing_rates' in locals() and 'time_bins' in locals() and 'session_data' in locals() and 'filtered_brain_areas' in session_data:\n",
    "    print(\"\\nPlotting firing rate heatmap...\")\n",
    "    plot_firing_rate_heatmap(firing_rates, session_data[\"filtered_brain_areas\"], time_bins)\n",
    "\n",
    "# Cell 28: Research Question Analysis Function\n",
    "def analyze_research_questions(integrated_data, session_idx, areas_of_interest):\n",
    "    \"\"\"\n",
    "    Analyze data specifically for the research questions:\n",
    "    1. Neural dynamics across MOs, basal ganglia, and prefrontal cortex in decision-making\n",
    "    2. Age-related differences in functional connectivity\n",
    "    \n",
    "    Parameters:\n",
    "    integrated_data : list\n",
    "        List of integrated session data\n",
    "    session_idx : int\n",
    "        Index of session to analyze\n",
    "    areas_of_interest : list\n",
    "        List of brain areas to analyze\n",
    "        \n",
    "    Returns:\n",
    "    analysis : dict\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Extract session data\n",
    "    session_data = extract_session_data(integrated_data, session_idx, areas_of_interest)\n",
    "    \n",
    "    if 'filtered_spike_times' not in session_data:\n",
    "        print(\"No spike data available for the specified areas\")\n",
    "        return None\n",
    "    \n",
    "    # Compute firing rates\n",
    "    firing_rates, time_bins = spike_times_to_firing_rate(session_data[\"filtered_spike_times\"])\n",
    "    \n",
    "    if firing_rates is None:\n",
    "        print(\"Failed to compute firing rates\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    analysis = {\n",
    "        \"session_idx\": session_idx,\n",
    "        \"areas_analyzed\": areas_of_interest,\n",
    "        \"time_bins\": time_bins\n",
    "    }\n",
    "    \n",
    "    # 1. Neural dynamics analysis\n",
    "    print(\"Analyzing neural dynamics...\")\n",
    "    \n",
    "    # Create trial info dictionary\n",
    "    trial_info = {}\n",
    "    for key in ['response', 'contrast_left', 'contrast_right']:\n",
    "        if key in session_data:\n",
    "            trial_info[key] = session_data[key]\n",
    "    \n",
    "    # Analyze neural dynamics\n",
    "    dynamics = analyze_neural_dynamics(firing_rates, session_data[\"filtered_brain_areas\"], trial_info)\n",
    "    analysis[\"neural_dynamics\"] = dynamics\n",
    "    \n",
    "    # 2. Functional connectivity analysis\n",
    "    print(\"Analyzing functional connectivity...\")\n",
    "    connectivity = compute_cross_regional_connectivity(firing_rates, session_data[\"filtered_brain_areas\"])\n",
    "    analysis[\"connectivity\"] = connectivity\n",
    "    \n",
    "    # 3. Decision-related activity\n",
    "    if 'response' in session_data:\n",
    "        print(\"Analyzing decision-related activity...\")\n",
    "        \n",
    "        # Split trials by response\n",
    "        left_trials = session_data['response'] == -1\n",
    "        right_trials = session_data['response'] == 1\n",
    "        \n",
    "        # Compute mean firing rates for each condition\n",
    "        left_rates = np.mean(firing_rates[:, left_trials, :], axis=1)  # neurons x time\n",
    "        right_rates = np.mean(firing_rates[:, right_trials, :], axis=1)  # neurons x time\n",
    "        \n",
    "        # Compute selectivity index for each neuron\n",
    "        selectivity = (right_rates - left_rates) / (right_rates + left_rates + 1e-10)  # Add small constant to avoid division by zero\n",
    "        \n",
    "        # Compute mean selectivity for each brain area\n",
    "        area_selectivity = {}\n",
    "        for area in np.unique(session_data[\"filtered_brain_areas\"]):\n",
    "            area_mask = session_data[\"filtered_brain_areas\"] == area\n",
    "            area_selectivity[area] = np.mean(selectivity[area_mask], axis=0)\n",
    "        \n",
    "        analysis[\"decision_selectivity\"] = {\n",
    "            \"neuron_selectivity\": selectivity,\n",
    "            \"area_selectivity\": area_selectivity\n",
    "        }\n",
    "    \n",
    "    # 4. LFP analysis if available\n",
    "    if 'filtered_lfp' in session_data and 'filtered_lfp_areas' in session_data:\n",
    "        print(\"Analyzing LFP data...\")\n",
    "        lfp_analysis = analyze_lfp(session_data[\"filtered_lfp\"], session_data[\"filtered_lfp_areas\"])\n",
    "        analysis[\"lfp_analysis\"] = lfp_analysis\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Cell 29: Example Usage - Research Question Analysis\n",
    "# Analyze data specifically for the research questions\n",
    "if 'integrated_data' in locals() and 'matching_sessions' in locals() and matching_sessions:\n",
    "    print(\"\\nAnalyzing data for research questions...\")\n",
    "    \n",
    "    # Choose the best session\n",
    "    best_session_idx = matching_sessions[0][\"session_idx\"]\n",
    "    \n",
    "    # Define areas of interest\n",
    "    areas_of_interest = ['MOs', 'ACB', 'CP', 'ACA', 'PL']  # MOs, basal ganglia, prefrontal cortex\n",
    "    \n",
    "    # Analyze data\n",
    "    research_analysis = analyze_research_questions(integrated_data, best_session_idx, areas_of_interest)\n",
    "    \n",
    "    if research_analysis is not None:\n",
    "        print(\"Research question analysis complete\")\n",
    "        print(f\"Analyzed session {research_analysis['session_idx']}\")\n",
    "        print(f\"Areas analyzed: {research_analysis['areas_analyzed']}\")\n",
    "        \n",
    "        # Plot decision selectivity\n",
    "        if 'decision_selectivity' in research_analysis:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            for i, (area, selectivity) in enumerate(research_analysis['decision_selectivity']['area_selectivity'].items()):\n",
    "                plt.plot(research_analysis['time_bins'], selectivity, label=area)\n",
    "            \n",
    "            plt.axvline(0, color='red', linestyle='--', label='Stimulus Onset')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Decision Selectivity')\n",
    "            plt.title('Decision Selectivity by Brain Area')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusion\n",
    "\n",
    "This notebook provides a comprehensive framework for loading, integrating, and analyzing the Steinmetz datasets:\n",
    "- steinmetz_st.npz (spike times)\n",
    "- steinmetz_lfp.npz (local field potentials)\n",
    "- steinmetz_wav.npz (waveforms)\n",
    "\n",
    "The key components of this notebook are:\n",
    "\n",
    "1. **Data Loading and Integration**\n",
    "   - Functions to download and load all three datasets\n",
    "   - Integration of data across datasets while preserving original structure\n",
    "   - Verification of data correspondence between datasets\n",
    "\n",
    "2. **Data Exploration and Selection**\n",
    "   - Functions to explore available sessions and brain areas\n",
    "   - Selection of sessions containing neurons from areas of interest\n",
    "   - Extraction and preprocessing of data for analysis\n",
    "\n",
    "3. **Neural Activity Analysis**\n",
    "   - Conversion of spike times to firing rates\n",
    "   - Analysis of LFP data in different frequency bands\n",
    "   - Visualization of neural activity patterns\n",
    "\n",
    "4. **Cross-Regional Connectivity Analysis**\n",
    "   - Computation of functional connectivity between brain regions\n",
    "   - Visualization of connectivity matrices\n",
    "   - Analysis of connectivity patterns in relation to behavior\n",
    "\n",
    "5. **Neural Dynamics Analysis**\n",
    "   - Dimensionality reduction of neural activity\n",
    "   - Visualization of neural trajectories in state space\n",
    "   - Comparison of neural dynamics between conditions\n",
    "\n",
    "6. **Research Question-Specific Analysis**\n",
    "   - Analysis of neural dynamics across MOs, basal ganglia, and prefrontal cortex\n",
    "   - Investigation of decision-related activity in these regions\n",
    "   - Framework for analyzing age-related differences in functional connectivity\n",
    "\n",
    "This notebook provides a foundation for addressing the research questions:\n",
    "1. How do neural dynamics across MOs, basal ganglia, and prefrontal cortex drive strategy selection and decision-making during visual discrimination tasks?\n",
    "2. How do age-related differences in functional connectivity between these regions influence cognitive processes and behavioral performance?\n",
    "\n",
    "Further analyses could include:\n",
    "- More detailed investigation of information flow between regions using Granger causality or other methods\n",
    "- Analysis of trial-by-trial variability in neural dynamics and its relationship to behavior\n",
    "- Comparison of neural activity patterns between young and old mice\n",
    "- Integration with behavioral models to link neural activity to cognitive processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
