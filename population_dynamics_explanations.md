# Population Dynamics Explanations

This document contains explanatory markdown cells for the "3_population_dynamics" notebook. These explanations cover the statistical methods, neuroscience concepts, and provide guidance on interpreting the results and plots.

## Introduction to Neural Population Dynamics

Neural population dynamics refers to the patterns of coordinated activity across many neurons over time. This approach moves beyond analyzing single neurons to understand how groups of neurons work together to process information, make decisions, and generate behavior.

**Key concepts in this notebook:**

- **Neural state space**: A conceptual space where each point represents the activity pattern of the entire neural population at a given moment
- **Dimensionality reduction**: Techniques like PCA that extract lower-dimensional representations of high-dimensional neural data
- **Neural trajectories**: Paths through neural state space that reveal how population activity evolves over time
- **Population coding**: How information about stimuli, decisions, and actions is encoded in distributed patterns of neural activity

**Why study population dynamics?**
- The brain operates as a coordinated system, not as individual neurons in isolation
- Many important neural computations emerge at the population level
- Population analyses can reveal computational principles that aren't visible when studying single neurons

**The Steinmetz dataset:**
This dataset contains neural recordings from mice performing a visual discrimination task. Mice had to determine whether a stimulus appeared on the left or right side of a screen and respond accordingly. The data includes neural activity from multiple brain regions, along with behavioral variables like stimulus properties and the animal's choices.

In this notebook, we'll analyze how neural populations represent task information and how their activity evolves throughout each trial, providing insight into the neural computations underlying sensory processing and decision-making.

## Understanding the Dataset Structure

In the Steinmetz dataset, neural activity was recorded from multiple brain regions while mice performed a visual discrimination task. The data we've loaded has the following structure:

- **firing_rates**: A matrix of shape (698, 99), where:
  - 698 is the number of time points (bins) across trials
  - 99 is the number of neurons recorded

The time bins span from -500ms to +500ms relative to stimulus onset, with 0ms representing the moment the visual stimulus was presented. Each value in the matrix represents the firing rate (in spikes/second) of a specific neuron at a specific time point.

This high-dimensional representation (99 neurons) makes it difficult to visualize and interpret the data directly. To address this, we'll use dimensionality reduction techniques to find a lower-dimensional representation that captures the essential features of the population activity.

## Dimensionality Reduction: Principal Component Analysis (PCA)

Dimensionality reduction is a crucial step in analyzing high-dimensional neural data. Principal Component Analysis (PCA) is one of the most common techniques used for this purpose.

**What is PCA?**
PCA transforms the high-dimensional neural activity into a new coordinate system, where:
- The first coordinate (first principal component) captures the direction of maximum variance in the data
- The second coordinate captures the direction of maximum remaining variance orthogonal to the first component
- And so on for additional components

**Why is standardization necessary?**
Before applying PCA, we standardize the data by:
1. Centering each neuron's activity around zero (subtracting the mean)
2. Scaling to unit variance (dividing by the standard deviation)

This ensures that neurons with higher firing rates don't dominate the analysis just because they're more active.

**Interpreting the explained variance plot:**
- The plot shows how much of the total variance in neural activity is captured by including a given number of principal components
- A steep initial rise followed by a plateau indicates that a small number of components capture most of the variance
- In this dataset, about 20 components are needed to explain 80% of the variance, suggesting the neural activity lies on a relatively low-dimensional manifold despite involving 99 neurons

This dimensionality reduction allows us to visualize neural trajectories and perform further analyses in a lower-dimensional space while preserving most of the relevant structure in the data.

## Neural State Space and Trajectories

Once we've reduced the dimensionality of our neural data using PCA, we can represent neural activity as a trajectory through a lower-dimensional "state space." This powerful approach allows us to visualize how patterns of activity across the entire neural population evolve over time.

**What is a neural state space?**
- Each point in this space represents the activity pattern of the entire neural population at a single moment in time
- The axes of this space correspond to the principal components (PCs) we identified earlier
- Movement through this space (trajectories) represents how population activity changes over time

**Understanding the metrics:**

1. **Speed in state space**:
   - Measures how quickly the pattern of neural activity is changing
   - Higher speeds indicate rapid transitions between neural states
   - The red vertical line at t=0 marks stimulus onset
   - Note how speed increases after stimulus presentation, reflecting the population's response to incoming sensory information

2. **Distance from starting point**:
   - Measures how far the current neural state is from the baseline (pre-stimulus) state
   - Increasing distance indicates the population is entering different activity states
   - The curve typically plateaus when the system reaches a new stable state

**Interpreting these patterns:**
- Rapid increases in speed after stimulus onset (t=0) reflect the neural population's immediate response to sensory input
- The gradual increase in distance from the starting point suggests the population transitions to a different activity state related to stimulus processing and decision-making
- The trajectory's behavior provides insight into the dynamics of neural computation during the task

These state space analyses help us understand how information flows through neural circuits and how different cognitive processes evolve over time.

## Neural Trajectories by Experimental Condition

One of the most powerful applications of state space analysis is comparing neural trajectories across different experimental conditions. By examining how neural activity patterns differ between conditions, we can understand how the brain represents task-relevant information.

**Understanding the plot:**
- Each line represents the average neural trajectory for a specific condition (left stimulus, right stimulus, or no contrast)
- The trajectories are projected onto the first two principal components (PC1 and PC2) for visualization
- Circles (â—‹) mark the starting points of trajectories, and X's mark the endpoints
- The separation between trajectories indicates that the neural population represents stimulus identity

**Key insights:**
- Different stimulus conditions lead to distinct neural trajectories, showing that the population encodes stimulus information
- The divergence of trajectories after stimulus onset (starting from similar initial states) demonstrates how sensory information is dynamically processed by the neural population
- The magnitude of separation between trajectories often correlates with the perceptual difference between stimuli

**Neuroscience interpretation:**
This analysis reveals how information about visual stimuli is encoded in the collective activity patterns of neurons. Rather than relying on single neurons to represent specific features, the brain uses distributed patterns across many neurons. The separation of trajectories in the state space shows that these population-level patterns contain reliable information that could be "read out" by downstream brain areas to guide behavior.

## Decoding Neural Representations

Now that we've visualized neural trajectories, we can quantitatively assess how well the neural population represents task-relevant information. Decoding analysis allows us to determine whether particular variables (like stimulus category or choice) can be "read out" from population activity.

**What is neural decoding?**
- Neural decoding uses statistical models to predict task variables from neural activity
- It helps quantify how much information about a variable is present in the neural population
- High decoding accuracy indicates the population strongly represents that variable

**The decoding process:**
1. We train a logistic regression model to predict stimulus category (left vs. right) from neural activity
2. We use the first 10 principal components at a specific time point (200ms after stimulus onset)
3. The decoder learns weights for each PC that best separate the stimulus categories
4. We evaluate performance by testing the model on held-out data

**Interpreting the results:**
- **Decoding accuracy**: The percentage of trials where the model correctly predicts the stimulus category from neural activity. An accuracy significantly above chance (50%) indicates the population reliably encodes stimulus information.
- **Decoder weights**: The importance of each principal component for distinguishing between stimulus categories. Components with larger absolute weights contribute more to the classification.

**Neuroscience insights:**
- Strong decoding performance suggests the brain uses distributed population codes to represent sensory information
- The weights reveal which activity patterns (principal components) are most informative about stimulus category
- This analysis bridges the gap between neural activity and behavior by showing how task-relevant information is represented in the brain

## Temporal Dynamics of Neural Coding

Neural processing is inherently dynamic, evolving over time as information flows through the brain. By examining how neural representations change across different time points, we can gain insight into the temporal dynamics of information processing.

**Understanding the temporal decoding analysis:**
- We train separate decoders at different time points throughout the trial
- Each decoder predicts stimulus category from neural activity at its specific time point
- The resulting curve shows how decodable the stimulus information is over time

**Interpreting the results:**
- **Baseline period (before t=0)**: Decoding accuracy should be near chance level (50%), as no stimulus information is available
- **Rising phase**: Accuracy increases as sensory information reaches and is processed by the recorded neural population
- **Peak accuracy**: The time point of maximum decodability indicates when the population representation is most informative
- **Late period**: Changes in accuracy may reflect shifts from sensory representation to decision-related or motor preparation signals

**Neuroscience insights:**
- The timing of information emergence in neural activity reveals the propagation of signals through the brain
- The latency to peak accuracy reflects processing delays in the neural circuit
- Sustained high accuracy after stimulus offset suggests the population maintains a memory trace of the stimulus
- Decreases in accuracy may indicate that the neural representation shifts to encode different aspects of the task (e.g., from stimulus to action)

This temporal decoding approach allows us to track the flow of information through neural circuits and understand how representations evolve during cognitive processing.

## Cross-Condition Generalization

A powerful way to understand neural representations is to test how they generalize across different conditions. This approach helps reveal whether the neural code for task variables is consistent or changes depending on other factors like behavioral choice.

**What is cross-condition generalization?**
- We train a decoder on one set of conditions (e.g., correct trials) and test it on another (e.g., error trials)
- Good generalization suggests a consistent neural representation across conditions
- Poor generalization indicates that the neural code depends on the specific condition

**The analysis approach:**
1. We identify different trial types: correct trials (stimulus and choice match) and error trials (stimulus and choice mismatch)
2. We train a stimulus decoder using only correct trials
3. We test this decoder on error trials to see if it can still predict the stimulus correctly
4. We compare this cross-condition performance to within-condition performance (training and testing on the same condition)

**Interpreting the results:**
- High cross-condition accuracy: The neural representation of stimulus is consistent regardless of the animal's choice
- Lower cross-condition than within-condition accuracy: The neural code for stimulus is influenced by the animal's choice
- Very low cross-condition accuracy: The neural representation might primarily encode the animal's choice rather than the stimulus

**Neuroscience insights:**
- This analysis helps distinguish between pure sensory representations and decision-influenced representations
- It can reveal how behavioral choices affect sensory encoding
- Understanding these interactions is crucial for comprehending how sensory information is transformed into decisions

## Statistical Properties of Neural Population Dynamics

Beyond visualizing neural trajectories, quantitative metrics can provide deeper insights into the structure and dynamics of population activity. These statistics help characterize how information is distributed across neurons and how neural representations evolve over time.

**Key statistical metrics:**

1. **Component variance**:
   - Shows how much variance in the population activity is captured by each principal component
   - The distribution of variance across components reveals the dimensionality structure of the data
   - A steep dropoff indicates a low-dimensional structure where a few components capture most of the variance

2. **Effective dimensionality**:
   - Quantifies the number of dimensions needed to capture the population dynamics
   - Calculated as the "participation ratio" of eigenvalues (total variance squared / sum of squared variances)
   - Lower values indicate simpler, more constrained dynamics; higher values suggest richer, more complex dynamics

3. **Temporal autocorrelation**:
   - Measures how similar the population state is at different time points compared to the initial state
   - Reveals the timescale of neural dynamics
   - Faster decay indicates more rapid evolution of neural states

**Interpreting the results:**
- The variance distribution across components reveals whether neural activity is constrained to a low-dimensional manifold or distributed across many dimensions
- The effective dimensionality provides a single number summarizing the complexity of neural dynamics
- The autocorrelation function shows how quickly the population state changes over time, with the inflection at t=0 (stimulus onset) indicating the impact of sensory input

**Neuroscience insights:**
- Low-dimensional dynamics (few components capturing most variance) suggest coordinated activity across neurons, potentially reflecting underlying computational principles
- The timescale of neural dynamics (from autocorrelation) may relate to the temporal integration properties of the circuit
- Changes in these statistics across brain regions can reveal differences in computational roles

These statistical measures provide quantitative frameworks for comparing neural dynamics across brain regions, tasks, and experimental conditions.

## Conclusion: What Population Dynamics Tell Us About Neural Computation

Throughout this notebook, we've analyzed neural population dynamics using different approaches. Here, we summarize the key insights these analyses provide about how the brain processes information and generates behavior.

**Core principles revealed by population dynamics:**

1. **Low-dimensional structure**: Despite recording from many neurons, neural activity is often constrained to a lower-dimensional manifold (approximately 20 dimensions in this dataset). This suggests that:
   - Neural populations act as coordinated units rather than independent elements
   - There may be simple underlying computational principles governing neural dynamics
   - The brain may use dimensionality reduction as a strategy to extract relevant information

2. **Dynamic representations**: Neural population activity evolves continuously over time, with:
   - Rapid responses to sensory input (seen in the speed and trajectory analyses)
   - Gradual transitions between different representational states
   - Maintenance of task-relevant information over time

3. **Distributed coding**: Information about stimuli and decisions is encoded across patterns of activity in many neurons, rather than in individual cells:
   - Different task variables are represented along different dimensions in the neural state space
   - The same neurons participate in representing multiple aspects of the task
   - Population-level readouts can extract task-relevant information reliably

**Implications for understanding brain function:**
- The brain uses population-level codes to represent and transform information
- Neural dynamics reflect the underlying computations that transform sensory inputs into decisions
- The low-dimensional structure of neural activity may reflect constraints that facilitate learning and reliable computation

**Future directions:**
- Comparing population dynamics across brain regions to understand information flow
- Linking population activity to specific computational models of decision-making
- Examining how population dynamics change with learning or altered task demands

Population dynamics analysis provides a powerful framework for understanding neural computation, moving beyond single-neuron approaches to reveal the collective principles that govern brain function.

## Expected Outcomes From Population Dynamics Analysis

When running this notebook, here's what you should expect to see and how to interpret the key results:

1. **PCA Explained Variance Plot**:
   - Expect to see a curve that rises quickly for the first few components and then levels off
   - Typically, ~20 components are needed to explain 80% of the variance in neural data
   - A steeper curve indicates more low-dimensional structure in the neural activity

2. **State Space Metrics**:
   - The speed plot should show a sharp increase shortly after stimulus onset (t=0)
   - The distance plot should show a gradual increase after stimulus onset, potentially plateauing
   - These patterns show how neural activity rapidly shifts to a new state after sensory input

3. **Neural Trajectories by Condition**:
   - Trajectories for different stimulus conditions should initially overlap and then diverge after stimulus onset
   - Left and right stimulus conditions should follow distinct paths in state space
   - The magnitude of separation often correlates with the strength of the sensory contrast

4. **Decoding Performance**:
   - Stimulus decoding accuracy should be significantly above chance (>50%)
   - Performance may vary across brain regions - sensory areas typically show higher accuracy
   - Decoder weights show which principal components carry task-relevant information

5. **Temporal Decoding**:
   - The accuracy curve should start near chance before stimulus onset
   - It should rise after stimulus presentation, potentially reaching a peak around 100-200ms
   - The timing of the peak and the shape of the curve can reveal processing delays in the brain

6. **Statistical Properties**:
   - The effective dimensionality is typically much lower than the number of recorded neurons
   - Autocorrelation should show a distinct change around stimulus onset
   - These metrics quantify the complexity and dynamics of neural population activity

These analyses together provide a comprehensive picture of how neural populations represent and process information during cognitive tasks, revealing computational principles that are not evident when studying single neurons in isolation. 